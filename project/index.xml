<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Code on Researcher profile</title>
    <link>https://wmkouw.github.io/project/</link>
    <description>Recent content in Code on Researcher profile</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0100</lastBuildDate>
    
	<atom:link href="https://wmkouw.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ForneyLab</title>
      <link>https://wmkouw.github.io/project/forneylab/</link>
      <pubDate>Thu, 07 Mar 2019 10:46:36 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/project/forneylab/</guid>
      <description>ForneyLab.jl is a Julia package for automatic generation of (Bayesian) inference algorithms. Given a probabilistic model, ForneyLab generates efficient Julia code for message-passing based inference [1]. It uses the model structure to generate an algorithm that consists of a sequence of local computations on a Forney-style factor graph (FFG) representation of the model [2].
Marco Cox, Thijs van de Laar en Bert de Vries are the ones who constructed ForneyLab.</description>
    </item>
    
    <item>
      <title>LibTLDA</title>
      <link>https://wmkouw.github.io/project/libtlda/</link>
      <pubDate>Sat, 09 Jun 2018 19:06:57 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/project/libtlda/</guid>
      <description>LibTLDA is a library of classifiers designed for domain adaptation and transfer learning, available in Matlab and Python. It started out as the collection of classifiers that I implemented during my PhD, but is now being expanded with more methods and algorithms.
Installation has been made easy, and there are demos to help you get started. Coders familiar with sci-kit will find it easy to pick up.
More information is available on its documentation page.</description>
    </item>
    
    <item>
      <title>YEAP16</title>
      <link>https://wmkouw.github.io/project/mfs-bonecnn/</link>
      <pubDate>Sat, 09 Sep 2017 19:12:54 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/project/mfs-bonecnn/</guid>
      <description>While I was at the Netherlands eScience Center, I worked on the Young eScientist Award 2016 project, together with its winner Maureen van Eijnatten and PhD student Jordi Minnema. They&amp;rsquo;re part of the 3D Innovation Lab under the department of Maxillo-Facial Surgery at the Vrije Universiteit medical center.
During the project, we developed a method for automatically detecting bone tissue in medical CT-scanner images. This method consisted of an artificial neural network, called a convolutional neural network (CNN), which looks at individual voxels in the image plus their surroundings.</description>
    </item>
    
    <item>
      <title>MRAI-net</title>
      <link>https://wmkouw.github.io/project/mrainet/</link>
      <pubDate>Thu, 01 Jun 2017 18:04:49 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/project/mrainet/</guid>
      <description>Suppose you have two sets of MR images, that were acquired using different scanners and/or different scanning protocols. One set of images is brighter and shows more tissue contrast than the other. One of them has annotations and the other does not. Tissue classifiers &amp;ndash; important to computer-aided diagnosis systems &amp;ndash; can be trained on the annotated data from one scanner, but when applied to data from the other scanner they will drastically under-perform.</description>
    </item>
    
    <item>
      <title>TCR</title>
      <link>https://wmkouw.github.io/project/tcpr/</link>
      <pubDate>Sun, 01 May 2016 18:05:01 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/project/tcpr/</guid>
      <description>In domain adaptation, one is always required to make assumptions on how the two domains relate to each other. However, depending on the domain dissimilarity, these assumptions can be quite strong. Furthermore, they are often hard to support, even with labeled target samples. Unfortunately, when an assumption is invalid, the classifier can adapt itself in ways that are detrimental to performance. Therefore, in practice, it can be hard to predict whether a domain-adaptive classifier will perform well for a given problem setting.</description>
    </item>
    
    <item>
      <title>FLDA</title>
      <link>https://wmkouw.github.io/project/flda/</link>
      <pubDate>Tue, 01 Dec 2015 18:05:05 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/project/flda/</guid>
      <description>Suppose that data is collected according to a set of features, such as blood pressure in clinical research or word frequencies in natural language processing. Now, in the source domain, all these features are measured, while in the target domain, some values are missing or absent (i.e. words are not used in the target context). In this case, one could capture the relationship between the domains using a probabilistic model, an approach we have called feature-level domain adaptation.</description>
    </item>
    
  </channel>
</rss>