<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research projects | Wouter M. Kouw</title>
    <link>https://wmkouw.github.io/project/</link>
      <atom:link href="https://wmkouw.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Research projects</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 01 Jan 2026 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wmkouw.github.io/media/icon_hu_3ae30e9b79d035f5.png</url>
      <title>Research projects</title>
      <link>https://wmkouw.github.io/project/</link>
    </image>
    
    <item>
      <title>AIM-TT</title>
      <link>https://wmkouw.github.io/project/aim-tt/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://wmkouw.github.io/project/aim-tt/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/aim-tt/aimtt-logo-tag_hu_dfb7e040b95a37f1.webp 400w,
               /project/aim-tt/aimtt-logo-tag_hu_b29dd60a9d115a69.webp 760w,
               /project/aim-tt/aimtt-logo-tag_hu_b867d75cfb48513e.webp 1200w&#34;
               src=&#34;https://wmkouw.github.io/project/aim-tt/aimtt-logo-tag_hu_dfb7e040b95a37f1.webp&#34;
               width=&#34;760&#34;
               height=&#34;146&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AiMTT aims to cultivate a highly skilled and diverse AI talent pool equipped to address the opportunities and challenges of AI in mobility, transport, and logistics. By combining real-world case studies with knowledge development, this initiative fosters deep expertise in the field.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Mobility, transport, and logistics face a multitude of challenges: traffic congestion, livability concerns, conflicts between user, operator, and public interests, space constraints, and safety risks during large-scale events. These challenges are further complicated by their deep interconnections, making them particularly difficult to resolve.&lt;/p&gt;
&lt;p&gt;While such complexities can be overwhelming for the human mind, they offer ideal use cases for artificial intelligence (AI). AI can process vast amounts of data in real time, provide accurate network assessments, calculate impacts in future scenarios, and optimize interventions. It also enhances our understanding of human behavior and the mobility system as a whole.&lt;/p&gt;
&lt;p&gt;Given these advantages, leveraging AI to address mobility challenges is a logical next step. Through AiMTT, we embrace a “learning by doing” approach to develop responsible, AI-driven solutions.&lt;/p&gt;
&lt;h2 id=&#34;vision-and-ambition&#34;&gt;Vision and Ambition&lt;/h2&gt;
&lt;p&gt;AiMTT stands for AI Learning Initiative for Multi-modal Traffic and Transportation. It operates under the umbrella of &lt;a href=&#34;https://aic4nl.nl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIC4NL&lt;/a&gt;, an organization dedicated to the responsible development and application of AI in the Netherlands. Ensuring responsible AI development is essential, as concerns about fairness, inclusivity, privacy, and human oversight continue to grow. Will AI-generated outcomes be equitable? Can privacy be safeguarded? How do we ensure that humans remain in control?&lt;/p&gt;
&lt;p&gt;AiMTT aims to address these critical questions by fostering a collaborative learning community that brings together experts from &lt;a href=&#34;https://aimtt.nl/about/partners&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;academia, industry, and government&lt;/a&gt;. Project partners will build, test, and refine AI applications, with ethical considerations—such as fairness, privacy, and human autonomy—at the forefront.&lt;/p&gt;
&lt;h2 id=&#34;learning-process&#34;&gt;Learning Process&lt;/h2&gt;
&lt;p&gt;AiMTT’s approach is grounded in practical application. AI solutions will be developed through &lt;a href=&#34;https://aimtt.nl/about/use-cases&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;seven real-world use cases&lt;/a&gt;, each designed to create tangible tools that can be directly implemented. Equally important is the learning process itself: identifying best practices, analyzing challenges, and refining AI applications based on real-world insights.&lt;/p&gt;
&lt;p&gt;To support this, the project will offer workshops, training programs, and co-creation sessions—ensuring continuous knowledge exchange and improvement.&lt;/p&gt;
&lt;p&gt;Through AiMTT, we are shaping the future of urban mobility by responsibly integrating AI to create smarter, safer, and more efficient transportation systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CONTACT-AI</title>
      <link>https://wmkouw.github.io/project/contact-ai/</link>
      <pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://wmkouw.github.io/project/contact-ai/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;. The International Labour Organisation reports over 300 million work-related accidents and diseases per year, with nearly 3 million being fatal (&lt;a href=&#34;https://www.ilo.org/resource/news/nearly-3-million-people-die-work-related-accidents-and-diseases&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ILO report&lt;/a&gt;). Embodied Artificially Intelligent (EAI) agents can reduce this drastically, by for example
inspecting construction sites or transporting cargo through hazardous areas. However, autonomously navigating unknown environments is difficult and requires adaptive decision-making. Suppose the agent detects a visually ambiguous obstacle: is it a crate that can be pushed away? Or a fence that needs to be navigated around? Rule-based algorithms and task-priority controllers could yield unsafe situations, while reinforcement learning (RL) requires enormous amounts of trial-and-error, potentially breaking the robot during training. The challenge is to design an EAI agent that cautiously and efficiently explores using multiple sensory modalities to find the best path through unknown terrain.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-figure-1-upon-detection-of-an-obstacle-external-uncertainty-vision-increases-this-uncertainty-will-be-transferred-to-internal-uncertainty-kinematic-the-agent-then-minimizes-kinematic-uncertainty-by-making-contact-with-the-obstacle&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 1. Upon detection of an obstacle, external uncertainty (vision) increases. This uncertainty will be transferred to internal uncertainty (kinematic). The agent then minimizes kinematic uncertainty by making contact with the obstacle.&#34; srcset=&#34;
               /project/contact-ai/CONTACTAI-uncertainty_hu_8dabb471d4a8d77a.webp 400w,
               /project/contact-ai/CONTACTAI-uncertainty_hu_743ee25080b61b50.webp 760w,
               /project/contact-ai/CONTACTAI-uncertainty_hu_23f95b37ed84a16b.webp 1200w&#34;
               src=&#34;https://wmkouw.github.io/project/contact-ai/CONTACTAI-uncertainty_hu_8dabb471d4a8d77a.webp&#34;
               width=&#34;760&#34;
               height=&#34;144&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 1. Upon detection of an obstacle, external uncertainty (vision) increases. This uncertainty will be transferred to internal uncertainty (kinematic). The agent then minimizes kinematic uncertainty by making contact with the obstacle.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution framework&lt;/strong&gt;: brain-inspired multi-modal switching dynamics. We believe an agent should use touch for exploration when vision cannot resolve ambiguity in its environment (Figure 1). Mechanically, we envision an agent that transfers visual uncertainty about the external world (what is this obstacle in front of me?) to kinematic uncertainty internally (what will happen if I move my leg?), and then reacts with actions that minimize uncertainty (e.g., gently push object with leg). To create such an agent, we take inspiration from natural embodied intelligence and computational neuroscience, specifically Active Inference. An active inference agent operates on beliefs (probability distributions over unknown variables) and updates these using variational Bayesian inference when new data is observed. Using quantified uncertainty, actions are balanced between exploration (maximizing information gain during data acquisition) and exploitation (reaching a goal). It has been demonstrated to be a powerful framework for planning and navigation. Uncertainty also leads to caution: slow careful movements when uncertainty is high and rapid targeted movements when uncertainty is low.&lt;/p&gt;
&lt;p&gt;We propose to design an active inference agent for a quadrupedal robot that incorporates visual perception, planning, decision-making and sensorimotor control (Figure 2). The active inference module learns two sets of dynamics: loco-motion and loco-manipulation. Visual perception is passed as a belief, expressed in terms of a factorized probability distribution, to the active inference module. Visual uncertainty is merged with the uncertainty in the loco-manipulation dynamics, akin to sensor fusion. When that uncertainty becomes large, the agent favours actions that minimize it in the future, such as manipulating the unknown object with its leg. Since the initial uncertainty will be high, the agent will make contact cautiously. Uncertainty shrinks with contact and a stronger action, such as pushing the object away, will be chosen, leading to a potentially improved locomotion path. In summary, the proposed active inference agent will use multiple modalities (vision, touch) to cautiously resolve ambiguity in the world and navigate the environment more robustly.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-figure-2-active-inference-agent-overview-vision-based-uncertainty-about-the-world-affects-kinematic-uncertainty-and-triggers-a-switch-to-loco-manipulation-exploring-the-world-through-cautious-touch-planning-implemented-as-message-passing-on-a-forney-style-factor-graph-edges-are-random-variables-nodes-are-operations-of-switching-autoregressive-models&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 2. Active inference agent overview. Vision-based uncertainty about the world affects kinematic uncertainty and triggers a switch to loco-manipulation, exploring the world through cautious touch. Planning implemented as message passing on a Forney-style factor graph (edges are random variables, nodes are operations) of switching autoregressive models.&#34; srcset=&#34;
               /project/contact-ai/CONTACTAI-system_hu_cf0cec2cf0b899c9.webp 400w,
               /project/contact-ai/CONTACTAI-system_hu_fd6a8499ff6b0cd4.webp 760w,
               /project/contact-ai/CONTACTAI-system_hu_29b8043979863b1.webp 1200w&#34;
               src=&#34;https://wmkouw.github.io/project/contact-ai/CONTACTAI-system_hu_cf0cec2cf0b899c9.webp&#34;
               width=&#34;760&#34;
               height=&#34;221&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 2. Active inference agent overview. Vision-based uncertainty about the world affects kinematic uncertainty and triggers a switch to loco-manipulation, exploring the world through cautious touch. Planning implemented as message passing on a Forney-style factor graph (edges are random variables, nodes are operations) of switching autoregressive models.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;. The agent will have a vision, a control and a decision-making module (Figure 2). The vision module runs a simultaneous localization and mapping algorithm as well as rudimentary object detection. The planning and navigation module will switch between locomotion and loco-manipulation. During locomotion, it generates targets for a gait controller and guides the robot along the planned path. During loco-manipulation, it plans a series of cautious contact-rich policies that maximize information gain on the object and whether it can be pushed away. We will use switching autoregressive models, that are explainable in terms of the effect of input sources on output prediction, and for which information gain can be calculated analytically. Computations are distributed by means of reactive message passing on a Forney-style factor graph. This ensures computation cost is small enough to run in-situ (e.g., Raspberry Pi + NVIDIA Jetson) on a low-cost quadrupedal robot platform (e.g., Petoi Bittle), which we aim to demonstrate as proof-of-concept.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FEPQuad</title>
      <link>https://wmkouw.github.io/project/fep-quad/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://wmkouw.github.io/project/fep-quad/</guid>
      <description>&lt;p&gt;We will design an artificially intelligent autonomous system for quadrupedal robot locomotion, using a novel paradigm from theoretical neuroscience called Active Inference (AIF). “Active” refers to selecting actions that reduce uncertainty within the probabilistic model of the world and “inference” refers to the use of variational Bayesian inference to update beliefs over unobserved variables in the model (e.g. parameters, states, noise, controls). AIF is a novel perspective on neural information processing and is intended to model cognition, for instance how rats explore small mazes in search of food. But it can also serve as a design principle for artificially intelligent autonomous systems (agents). These can be applied to signal processing systems (e.g. adaptively calibrating hearing aids), control systems (e.g. identifying electro-mechanical systems) or robotics (e.g. learning to grasp). However, bringing AIF to engineering is far from trivial. There are many technical challenges, such as how to account for strong non-linearities, how to deal with high degrees of freedom of moving parts or how to include practical constraints to avoid breaking hardware.&lt;/p&gt;
&lt;p&gt;Why Active Inference? It represents a series of technical advantages over the current state-of-the-art in AI methodologies. Deep learning is a popular framework with impressive applications, but is not without its limitations. Firstly, it requires huge amounts of data to “discover” structure. AIF is a hybrid of model-driven and data-driven learning, which means it can rely on prior knowledge when data is scarce. Crucially, deep learning methods only perform well after an experienced designer has extensively tuned the network’s architecture, regularized its complexity and tried various optimizers. In AIF, there are fewer model parameters, regularization arises naturally through prior distributions and optimization is not an issue. To train deep neural networks, you need expensive hardware (GPU) with a sizeable carbon footprint. AIF agents require less computation power and are more suited to embedded electronics. Last but not least, when deep learning is applied to control (deep reinforcement learning), the engineer must design a “reward function” that indicates the value of actions. This function is hard to design, leading to misbehaving agents. AIF agents do not suffer from this problem because rewards arise implicitly from the probabilistic model. These properties are all nice, but the most important argument for AIF is that it represents a principled way to design intelligent systems: instead of hacking something together based on task-specific cost functions, we now have a first-principle-based framework of perception, decision-making, planning and action.&lt;/p&gt;
&lt;p&gt;Why quadrupedal walking robots? Because unlike wheeled robots, walkers can step over objects and climb stairs. Unlike drones, they can enter confined spaces and operate for extended periods of time. In theory, quadrupeds are highly agile. In practice, learning to walk is such a complex challenge that they often fail to live up to their potential. Modern AI has accelerated legged robotics to the point that quadrupeds now walk relatively smoothly. Companies such as Boston Dynamics, ANYbotics and Unitree are developing commercial products for semi-autonomous site inspection and maintenance. But their controllers still rely heavily on deep learning. Our AIF agent will make it much easier to teach legged robots to walk.&lt;/p&gt;
&lt;p&gt;How will the proposed agent work? AIF agents are based on a probabilistic graphical model expressing the dependence of observed and unknown variables through conditional distributions. For dynamical systems, there are leaf nodes representing initial conditions that are specified as “prior beliefs”. One can estimate the posterior distributions for the unknowns (e.g. states, parameters, noises) through Bayesian inference, usually in the form of a message-passing algorithm. Sometimes, the integrals involved are intractable. Variational inference solves this by approximating the posteriors with a simpler “recognition model”. AIF agents are essentially a form of variational Bayesian inference on probabilistic graphical models of dynamic systems that alternate between solving a signal processing (perception) and a control (action) problem to reach a goal.&lt;/p&gt;
&lt;p&gt;The position is supported by the Sectorplan Techniek of the Dutch Ministry of Education, Culture and Science and the Eindhoven Artificial Intelligence Systems Institute.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BayesBrain</title>
      <link>https://wmkouw.github.io/project/bayesbrain/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://wmkouw.github.io/project/bayesbrain/</guid>
      <description>&lt;p&gt;Computation in biological brain tissue consumes several orders of magnitude less power than silicon-based systems. Motivated by this fact, this project aims to develop the world’s first hybrid neuro-in-silico Artificial Intelligence (AI) computer, introducing a fundamentally new paradigm of AI computing. In this high-risk high-gain project, we will combine an in-silico Bayesian control agent (BCA) with neural tissue hosted by a microfluidic Brain-on-Chip (BoC) that together form a hybrid learning system capable of solving real-world AI problems.&lt;/p&gt;
&lt;p&gt;All computation and communication inside and between the BCA and BoC will be governed by the Free Energy Principle, which is both the leading neuroscientific theory for describing biological neuronal processes and supports a variational Bayesian machine learning interpretation. We will start by developing a pure silicon-based BCA that learns to balance an inverted pendulum, implemented by free energy minimization on a factor graph. Next, we will replace successively larger parts of the factor graph with biological neural circuits of a microfluidic multi-compartment BoC device. The biological network will be trained by electrical stimulation orchestrated by the synthetic Bayesian agent. For the communication between these two units, we will design and realize a novel communication protocol making use of existing software being applied in readout and event sorting for Calcium imaging and multi-electrode array data, such as MEAViewer, CALIMA, NetCal and SpikeHunter. By upscaling the number of replaced sub-circuits, we aim to provide a proof-of-concept and to lay the basis for ultra-low power hybrid brain-on-chip AI computing.&lt;/p&gt;
&lt;p&gt;This position is supported by the Exploratory Multidiscplinary AI Research Program of the Eindhoven Artificial Intelligence Systems Institute.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SeqDAIS</title>
      <link>https://wmkouw.github.io/project/seqdais/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://wmkouw.github.io/project/seqdais/</guid>
      <description>&lt;p&gt;The capacity of supervised learning systems to generalize to new examples is inherently limited by the collected data. If it is not an accurate reﬂection of the population, then the system will not perform well. In particular, if the collected data is biased, which means that one observes certain examples more often than normal, then the system can be misled into thinking that certain outcomes are also more likely to occur. Although the system might make accurate diagnoses for new patients arriving to that particular hospital, it will make inaccurate diagnoses for patients arriving to 1a diﬀerent hospital. The diﬀerences between patient populations might be due to regional diﬀerences such as exercise culture, but the fact that one observes for instance older patients more than normal means that data collected from that hospital is biased with respect to the total human population.&lt;/p&gt;
&lt;p&gt;Handling biased samples from populations is a problem that has long been studied in statistics and econometrics. Although a number of techniques have been proposed to improve supervised learning systems trained on biased data, things have changed with the tremendous increase in computational power in the last 20 years. The ﬁeld has advanced to the point where we ask the question whether it is possible to generalize to particular target populations as well. Can we adapt a supervised learning system trained on adult human heart disease patients to make accurate decisions for infant heart disease patients?&lt;/p&gt;
&lt;p&gt;Work from the last 10 years has looked at incorporating unlabeled data from these target populations. With this additional information, systems can recognize changes in data properties, ﬁnd correspondences between populations and adapt their decisions accordingly. Successful adaptation is deﬁned as an improvement over the performance of the original system. Nonetheless, the analysis of this problem is not complete, and it is not clear which conditions have to be fulﬁlled in order for the system to perform well. It seems that in cases where it is diﬃcult to describe how two populations relate to each other, adaptive systems suﬀer from high variability. They are highly uncertain about their decisions and often wrong. However, it seems that the more similar the populations are, the likelier it is that the system adapts well. It would, for example, be easier for the system to adapt to predict heart disease in adolescents based on data from adults, then it would for the system to adapt to infants. However, here might lie a potentially crucial insight: can we design a system that ﬁrst adapts to the closest population and only then adapts to the ﬁnal target population?
In other words, a system that sequentially adapts?&lt;/p&gt;
&lt;p&gt;This position is supported by a Niels Stensen Fellowship grant, offered by the Niels Stensen Stichting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DAPR</title>
      <link>https://wmkouw.github.io/project/dapr/</link>
      <pubDate>Mon, 01 Sep 2014 00:00:00 +0000</pubDate>
      <guid>https://wmkouw.github.io/project/dapr/</guid>
      <description>&lt;p&gt;Artificial intelligence, in particular machine learning and pattern recognition, is concerned with teaching computer systems to perform tasks. Tasks such as autonomous driving, recognizing tumors in medical images, or detecting suspicious packages in airports. Such systems learn by observing examples, i.e. data, and forming a mathematical description of what types of variations occur, i.e. a statistical model. For new input, the system computes the most likely output and makes a decision accordingly. As a scientific field, it is situated between statistics and and algorithmics. As a technology, it has become a very powerful tool due to the massive amounts of data being collected and the drop in the cost of computation.&lt;/p&gt;
&lt;p&gt;However, obtaining &lt;em&gt;enough&lt;/em&gt; data is still very difficult. There are often substantial financial, operational or ethical considerations in collecting data. The majority of research in machine learning deals with constraints on the amount, the labeling and the types of data that are available. One such constraint is that it is only possible to collect labeled data from one population, or domain, but the goal is to make decisions for another domain. It is unclear under which conditions this will be possible, which inspires the research question: when and how can a classification algorithm generalize from a source domain to a target domain?&lt;/p&gt;
&lt;p&gt;My research has looked at different approaches to domain adaptation. Firstly, we have asked some critical questions on whether the standard approaches to model validation still hold in the context of different domains. As a result, we have proposed a means to reduce uncertainty in the validation risk estimator, but that does not solve the problem completely. Secondly, we modeled the transfer from source to target domain using parametric families of distributions, which works well in simple contexts such as feature dropout at test time. Thirdly, we looked at a more practical problem: tissue classifiers trained on data from one MRI scanner degrade when applied to data from another scanner due to acquisition-based variations. We tackled this problem by learning a representation for which detrimental variations are minimized while maintaining tissue contrast. Finally, considering that many approaches fail in practice because their assumptions are not met, we designed a parameter estimator that never performs worse than the naive non-adaptive classifier.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
