<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Researcher profile on Researcher profile</title>
    <link>https://wmkouw.github.io/</link>
    <description>Recent content in Researcher profile on Researcher profile</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 27 May 2021 12:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Message passing-based inference for time-varying autoregressive models</title>
      <link>https://wmkouw.github.io/publication/tvar-ffg/</link>
      <pubDate>Thu, 27 May 2021 12:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/tvar-ffg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust domain-adaptive discriminant analysis</title>
      <link>https://wmkouw.github.io/publication/tcpr/</link>
      <pubDate>Tue, 25 May 2021 09:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/tcpr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Target robust discriminant analysis</title>
      <link>https://wmkouw.github.io/publication/trda/</link>
      <pubDate>Thu, 21 Jan 2021 10:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/trda/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Target robust discriminant analysis</title>
      <link>https://wmkouw.github.io/talk/s&#43;sspr2020/</link>
      <pubDate>Thu, 21 Jan 2021 09:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/s&#43;sspr2020/</guid>
      <description>&lt;p&gt;I presented part of my work on desigining safe, robust domain-adaptive estimators (see &lt;a href=&#34;https://wmkouw.github.io/publication/trda/&#34; target=&#34;_blank&#34;&gt;publication&lt;/a&gt;). This workshop paper focused on the performance improvement guarantee for the specific case of discriminant analyses.&lt;/p&gt;

&lt;p&gt;S+SSPR 2020 was an online event due to the corona pandemic. I recorded a video beforehand and presented my paper in a Zoom session.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variational Bayes for signal processing</title>
      <link>https://wmkouw.github.io/talk/sioux-seminar2020/</link>
      <pubDate>Thu, 05 Nov 2020 15:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/sioux-seminar2020/</guid>
      <description>&lt;p&gt;This is a talk for the Optimization / Machine Learning Competence group at Sioux Technologies. I presented the probabilistic derivations for the Kalman filter, how to extend those to variational Bayesian filters and some of BIASlab&amp;rsquo;s work on signal processing systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Online system identification using free energy minimisation</title>
      <link>https://wmkouw.github.io/talk/iwai2020/</link>
      <pubDate>Sun, 13 Sep 2020 09:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/iwai2020/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://wmkouw.github.io/posters/IWAI2020_wmkouw.png&#34; alt=&#34;Poster IWAI2020&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Online system identification in a Duffing oscillator by free energy minimisation</title>
      <link>https://wmkouw.github.io/publication/nsi-silverbox/</link>
      <pubDate>Tue, 01 Sep 2020 12:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/nsi-silverbox/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The data representativeness criterion</title>
      <link>https://wmkouw.github.io/publication/drc/</link>
      <pubDate>Tue, 11 Aug 2020 10:21:16 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/drc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Online variational message passing in hierarchical autoregressive models</title>
      <link>https://wmkouw.github.io/publication/vmphar/</link>
      <pubDate>Sun, 21 Jun 2020 12:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/vmphar/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AR-HGF</title>
      <link>https://wmkouw.github.io/project/ar-hgf/</link>
      <pubDate>Thu, 11 Jun 2020 13:26:41 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/project/ar-hgf/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;We address the problem of online Bayesian state and parameter tracking in autoregressive (AR) models with time-varying process noise variance. The involved marginalization and expectation integrals cannot be analytically solved. Moreover, the online tracking constraint makes sampling and batch learning methods unsuitable for this problem. We propose a hybrid variational message passing algorithm that robustly tracks the time-varying dynamics of the latent states, AR coefficients and process noise variance. Since message passing in a factor graph is a highly modular inference approach, the proposed methods easily extend to other non-stationary dynamic modeling problems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian joint state and parameter tracking in autoregressive models</title>
      <link>https://wmkouw.github.io/publication/vmpar-hgf/</link>
      <pubDate>Thu, 11 Jun 2020 12:00:00 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/vmpar-hgf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Schedule-free variational message passing for Bayesian filtering</title>
      <link>https://wmkouw.github.io/talk/neuromatch2020/</link>
      <pubDate>Tue, 31 Mar 2020 12:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/neuromatch2020/</guid>
      <description>&lt;p&gt;In Bayesian filtering, states and parameters of probabilistic state-space models are inferred in an online manner. Using the Free Energy Principle, the state-space model is cast to a generative model &lt;em&gt;p&lt;/em&gt; and the posterior distributions of interest are approximated using recognition distributions or beliefs &lt;em&gt;q&lt;/em&gt;. The factorisation of state-space models into state transitions and observation likelihoods over time supports forming a factor graph and performing inference via message passing.&lt;/p&gt;

&lt;p&gt;Tools for message passing on factor graphs typically employ a scheduling procedure, in which a separate algorithm or compiler takes the model description and returns &lt;em&gt;which&lt;/em&gt; nodes should pass messages &lt;em&gt;where&lt;/em&gt; at &lt;em&gt;what&lt;/em&gt; time. This can be sufficiently expensive to form a bottleneck. Moreover, it&amp;rsquo;s not a biologically plausible mechanism for governing message passing. I explore the possibility of passing messages without a scheduler. A designated terminal node should pass an initial message, which will arrive at an initial variable. The corresponding belief is updated, a local Free Energy is computed and the belief is emitted to neighbouring factor nodes. From there on out, whenever an updated belief arrives at a factor node, the node fires messages to all other variables if the local Free Energy surpasses a threshold. If not, the node becomes silent.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TUe 5SSD0 BMLIP</title>
      <link>https://wmkouw.github.io/teaching/tue-5ssd0/</link>
      <pubDate>Thu, 16 Jan 2020 13:26:41 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/teaching/tue-5ssd0/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://biaslab.github.io/teaching/bmlip/&#34; target=&#34;_blank&#34;&gt;Bayesian Machine Learning &amp;amp; Information Processing&lt;/a&gt; (BMLIP) is an elective course in the Electrical Engineering Master Program of the TU Eindhoven.&lt;/p&gt;

&lt;p&gt;BMLIP covers the fundamentals of the Bayesian (i.e., probabilistic) approach to machine learning and information processing systems. Firstly, we discuss many useful models including common regression and classification methods, Gaussian mixture models, hidden Markov models and Kalman filters. Secondly, we teach Expectation-Maximization (EM), Variational Bayes (VB), Variational Message Passing (VMP) and basic Monte Carlo sampling. Lastly, we discuss intelligent agents that learn purposeful behavior from interactions with their environment.&lt;/p&gt;

&lt;p&gt;I teach the mini-course on Probabilistic Programming, where we familiarize students with software for automatic inference in probabilistic models.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SSA-NLP</title>
      <link>https://wmkouw.github.io/project/ssa-nlp/</link>
      <pubDate>Fri, 15 Nov 2019 13:26:41 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/project/ssa-nlp/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Language evolves over time in many ways relevant to natural language processing tasks. For example, recent occurrences of tokens &amp;lsquo;BERT&amp;rsquo; and &amp;lsquo;ELMO&amp;rsquo; in publications refer to neural network architectures rather than persons [&lt;a href=&#34;https://www.aclweb.org/anthology/N19-1423/&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1903.05987&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;]. This type of temporal signal is typically overlooked, but is important if one aims to deploy a machine learning model over an extended period of time. In particular, language evolution causes data drift between time-steps in sequential decision-making tasks. Examples of such tasks include prediction of paper acceptance for yearly conferences (regular intervals) or author stance prediction for rumours on Twitter (irregular intervals). We tackle data drift by sequentially aligning learned representations. We argue that, due to its low computational expense, sequential alignment is a practical solution to dealing with language evolution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Back to the future - temporal adaptation of text representations</title>
      <link>https://wmkouw.github.io/publication/seqrum/</link>
      <pubDate>Mon, 11 Nov 2019 06:00:40 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/seqrum/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
