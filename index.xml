<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Researcher profile on Researcher profile</title>
    <link>https://wmkouw.github.io/</link>
    <description>Recent content in Researcher profile on Researcher profile</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Tue, 31 Mar 2020 12:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Online variational message passing in hierarchical autoregressive models</title>
      <link>https://wmkouw.github.io/publication/vmphar/</link>
      <pubDate>Tue, 31 Mar 2020 12:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/vmphar/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Schedule-free variational message passing for Bayesian filtering</title>
      <link>https://wmkouw.github.io/talk/neuromatch2020/</link>
      <pubDate>Tue, 31 Mar 2020 12:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/neuromatch2020/</guid>
      <description>&lt;p&gt;In Bayesian filtering, states and parameters of probabilistic state-space models are inferred in an online manner. Using the Free Energy Principle, the state-space model is cast to a generative model &lt;em&gt;p&lt;/em&gt; and the posterior distributions of interest are approximated using recognition distributions or beliefs &lt;em&gt;q&lt;/em&gt;. The factorisation of state-space models into state transitions and observation likelihoods over time supports forming a factor graph and performing inference via message passing.&lt;/p&gt;

&lt;p&gt;Tools for message passing on factor graphs typically employ a scheduling procedure, in which a separate algorithm or compiler takes the model description and returns &lt;em&gt;which&lt;/em&gt; nodes should pass messages &lt;em&gt;where&lt;/em&gt; at &lt;em&gt;what&lt;/em&gt; time. This can be sufficiently expensive to form a bottleneck. Moreover, it&amp;rsquo;s not a biologically plausible mechanism for governing message passing. I explore the possibility of passing messages without a scheduler. A designated terminal node should pass an initial message, which will arrive at an initial variable. The corresponding belief is updated, a local Free Energy is computed and the belief is emitted to neighbouring factor nodes. From there on out, whenever an updated belief arrives at a factor node, the node fires messages to all other variables if the local Free Energy surpasses a threshold. If not, the node becomes silent.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian joint state and parameter tracking in autoregressive models</title>
      <link>https://wmkouw.github.io/publication/vmpar-hgf/</link>
      <pubDate>Wed, 04 Mar 2020 12:00:00 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/vmpar-hgf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The data representativeness criterion</title>
      <link>https://wmkouw.github.io/publication/drc/</link>
      <pubDate>Thu, 27 Feb 2020 10:21:16 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/drc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TUe 5SSD0 BMLIP</title>
      <link>https://wmkouw.github.io/teaching/tue-5ssd0/</link>
      <pubDate>Thu, 16 Jan 2020 13:26:41 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/teaching/tue-5ssd0/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://biaslab.github.io/teaching/bmlip/&#34; target=&#34;_blank&#34;&gt;Bayesian Machine Learning &amp;amp; Information Processing&lt;/a&gt; (BMLIP) is an elective course in the Electrical Engineering Master Program of the TU Eindhoven.&lt;/p&gt;

&lt;p&gt;BMLIP covers the fundamentals of the Bayesian (i.e., probabilistic) approach to machine learning and information processing systems. Firstly, we discuss many useful models including common regression and classification methods, Gaussian mixture models, hidden Markov models and Kalman filters. Secondly, we teach Expectation-Maximization (EM), Variational Bayes (VB), Variational Message Passing (VMP) and basic Monte Carlo sampling. Lastly, we discuss intelligent agents that learn purposeful behavior from interactions with their environment.&lt;/p&gt;

&lt;p&gt;I teach the mini-course on Probabilistic Programming, where we familiarize students with software for automatic inference in probabilistic models.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ssa-nlp</title>
      <link>https://wmkouw.github.io/project/ssa-nlp/</link>
      <pubDate>Fri, 15 Nov 2019 13:26:41 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/project/ssa-nlp/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Language evolves over time in many ways relevant to natural language processing tasks. For example, recent occurrences of tokens &amp;lsquo;BERT&amp;rsquo; and &amp;lsquo;ELMO&amp;rsquo; in publications refer to neural network architectures rather than persons [&lt;a href=&#34;https://www.aclweb.org/anthology/N19-1423/&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1903.05987&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;]. This type of temporal signal is typically overlooked, but is important if one aims to deploy a machine learning model over an extended period of time. In particular, language evolution causes data drift between time-steps in sequential decision-making tasks. Examples of such tasks include prediction of paper acceptance for yearly conferences (regular intervals) or author stance prediction for rumours on Twitter (irregular intervals). We tackle data drift by sequentially aligning learned representations. We argue that, due to its low computational expense, sequential alignment is a practical solution to dealing with language evolution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Back to the future - sequential alignment of text representations</title>
      <link>https://wmkouw.github.io/publication/seqrum/</link>
      <pubDate>Mon, 11 Nov 2019 06:00:40 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/seqrum/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust importance-weighted cross-validation under sample selection bias</title>
      <link>https://wmkouw.github.io/talk/mlsp2019/</link>
      <pubDate>Fri, 11 Oct 2019 09:11:43 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/mlsp2019/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://wmkouw.github.io/posters/MLSP2019_wmkouw.png&#34; alt=&#34;Poster MLSP2019&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A review of domain adaptation without target labels</title>
      <link>https://wmkouw.github.io/publication/review/</link>
      <pubDate>Mon, 07 Oct 2019 16:13:59 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/review/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sequential domain-adaptive machine learning</title>
      <link>https://wmkouw.github.io/talk/nsf2019/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/talk/nsf2019/</guid>
      <description>&lt;p&gt;This poster recaps two collaboration projects I did during my time as Niels Stensen Fellow at the University of Copenhagen. The main point of my proposal was to study &amp;ldquo;sequential domain adaptation&amp;rdquo;. The left column describes an application to &amp;ldquo;spatial sequences&amp;rdquo;, i.e., multi-site biomedical imaging, and the right column describes an application to &amp;ldquo;temporal sequences&amp;rdquo;, i.e. natural language processing over data collected in snapshots over time. The main take-home message from the work in this Fellowship is that domain adaptation is a solution to training machine learning models under sampling bias and that sequential adaptation allows for updating.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wmkouw.github.io/posters/NSF2019_wmkouw.png&#34; alt=&#34;Poster NSF2019&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robust importance-weighted cross-validation under sample selection bias</title>
      <link>https://wmkouw.github.io/publication/robust-iwxval/</link>
      <pubDate>Tue, 30 Jul 2019 06:34:24 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/robust-iwxval/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cross-center smoothness prior for Bayesian image segmentation</title>
      <link>https://wmkouw.github.io/talk/ipmi2019/</link>
      <pubDate>Sun, 02 Jun 2019 09:11:43 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/ipmi2019/</guid>
      <description>&lt;p&gt;A simple idea: learn how smooth medical image segmentations are supposed to look like and use this knowledge as a prior for Bayesian image segmentation. That way, you avoid learning a different mapping from scan to segmentation for each scanner while still learning from data at other medical centers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wmkouw.github.io/posters/IPMI2019_wmkouw.png&#34; alt=&#34;Poster IPMI2019&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Are all practical problems interesting?</title>
      <link>https://wmkouw.github.io/post/practical_problems/</link>
      <pubDate>Wed, 22 May 2019 16:58:17 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/post/practical_problems/</guid>
      <description>&lt;p&gt;On many occasions, I have seen researchers being recommended to work on practical problems. For example, instead of looking for a neural network architecture that has never been proposed before, design a model for tracking a particular stage of cancer development. A first argument is that this makes a stronger impact on clinical research and society. The applied researcher would be happy, but a theorist would argue that the result is too specific to generalize to other diseases, and is therefore less interesting. But new problem settings can reveal new behavior of a model or method. Therefore, it is also in the interest of the theorist to consider practical problems.&lt;/p&gt;

&lt;!-- In fields revolving around data analysis, methodological research and algorithm design, focusing on practical problems often means working with data collected by domain experts instead of simulations. The argument is that even though a model / method works well for synthetic data, it will not necessarily work well for natural data. Since we are ultimately interested in solving real-world problems, validation on synthetic data is not sufficient. --&gt;

&lt;!-- I think that argument is sound, but the recommendation to work on practical problems does not follow.  --&gt;

&lt;p&gt;But I argue that not all practical problems are interesting. Some exist purely because people cannot communicate well or agree on something. An example is the use of different protocols: suppose there are two teams working on genomic data analysis &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3880143/&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;. Both use different protocols, because each thinks theirs is better (which is probably true since both define different metrics). Although each team produces excellent work, their data sets cannot be easily integrated due to the differences in protocol. Without large cross-center data sets, it becomes difficult to fit more complex models, and answer more difficult questions.&lt;/p&gt;

&lt;p&gt;The analyst is often tasked with automatically integrating data sets, or designing models that cope with a variety of data sets. The frustrating thing is that this task is &lt;em&gt;unnecessary&lt;/em&gt;; it would be resolved if both teams would use coherent protocols. Certain bodies of government are aware of this, such as the EU, but it is proving difficult to convince people to adhere to a policy &lt;a href=&#34;https://publications.europa.eu/en/publication-detail/-/publication/7769a148-f1f6-11e8-9982-01aa75ed71a1/language-en/format-PDF/source-80611283&#34; target=&#34;_blank&#34;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Practical problems are not always &lt;em&gt;interesting&lt;/em&gt;: some reveal more about people and their choices, than about the behavior of a model or method.&lt;/p&gt;

&lt;p&gt;References: &lt;br /&gt;
[1] &lt;a href=&#34;Tackling the widespread and critical impact of batch effects in high-throughput data&#34; target=&#34;_blank&#34;&gt;Tackling the widespread and critical impact of batch effects in high-throughput data. Leek et al., Nature Reviews, 2010.&lt;/a&gt;&lt;br /&gt;
[2] &lt;a href=&#34;https://publications.europa.eu/en/publication-detail/-/publication/7769a148-f1f6-11e8-9982-01aa75ed71a1/language-en/format-PDF/source-80611283&#34; target=&#34;_blank&#34;&gt;Turning FAIR into reality: Final report and action plan from the European Commission expert group on FAIR data. Directorate-General for Research and Innovation (European Commission), EU publications, 2018.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MR Acquisition-invariant representation learning</title>
      <link>https://wmkouw.github.io/talk/isbi2019/</link>
      <pubDate>Fri, 12 Apr 2019 09:11:43 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/isbi2019/</guid>
      <description>&lt;p&gt;This conference poster refers back to the project on accounting for variation in medical images due to MR acquisition protocol. Acquisition variation is caused by different vendors, mechanical calibrations and environmental effects at different medical centers, and hamper generalization of medical image processing techniques that employ machine learning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wmkouw.github.io/posters/ISBI2019_wmkouw.png&#34; alt=&#34;Poster ISBI2019&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ForneyLab.jl</title>
      <link>https://wmkouw.github.io/project/forneylab/</link>
      <pubDate>Thu, 07 Mar 2019 10:46:36 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/project/forneylab/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;ForneyLab.jl is a Julia package for automatic generation of (Bayesian) inference algorithms. Given a probabilistic model, ForneyLab generates efficient Julia code for message-passing based inference &lt;a href=&#34;https://arxiv.org/abs/1811.03407&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;. It uses the model structure to generate an algorithm that consists of a sequence of local computations on a Forney-style factor graph (FFG) representation of the model &lt;a href=&#34;https://ieeexplore.ieee.org/document/4282128/&#34; target=&#34;_blank&#34;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Marco Cox, Thijs van de Laar en Bert de Vries are the ones who constructed ForneyLab. At the moment, I contribute to the package through support with numerical stability, ease-of-use, and documentation. In the future, I will be adding functionality.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
