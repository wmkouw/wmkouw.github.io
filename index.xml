<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Researcher profile on Researcher profile</title>
    <link>https://wmkouw.github.io/</link>
    <description>Recent content in Researcher profile on Researcher profile</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Fri, 12 Apr 2019 09:11:43 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>MR Acquisition-invariant representation learning</title>
      <link>https://wmkouw.github.io/talk/isbi2019/</link>
      <pubDate>Fri, 12 Apr 2019 09:11:43 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/isbi2019/</guid>
      <description>&lt;p&gt;This conference poster refers back to the project on accounting for variation in medical images due to MR acquisition protocol. Acquisition variation is caused by different vendors, mechanical calibrations and environmental effects at different medical centers, and hamper generalization of medical image processing techniques that employ machine learning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>forneylab</title>
      <link>https://wmkouw.github.io/project/forneylab/</link>
      <pubDate>Thu, 07 Mar 2019 10:46:36 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/project/forneylab/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;ForneyLab.jl is a Julia package for automatic generation of (Bayesian) inference algorithms. Given a probabilistic model, ForneyLab generates efficient Julia code for message-passing based inference &lt;a href=&#34;https://arxiv.org/abs/1811.03407&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;. It uses the model structure to generate an algorithm that consists of a sequence of local computations on a Forney-style factor graph (FFG) representation of the model &lt;a href=&#34;https://ieeexplore.ieee.org/document/4282128/&#34; target=&#34;_blank&#34;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Marco Cox, Thijs van de Laar en Bert de Vries are the ones who constructed ForneyLab. At the moment, I contribute to the package through support with numerical stability, ease-of-use, and documentation. In the future, I will be adding functionality.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A cross-center smoothness prior for variational Bayesian brain tissue segmentation</title>
      <link>https://wmkouw.github.io/publication/infoprior/</link>
      <pubDate>Wed, 06 Mar 2019 14:13:32 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/infoprior/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ccsmooth</title>
      <link>https://wmkouw.github.io/project/info-prior/</link>
      <pubDate>Fri, 01 Mar 2019 22:28:22 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/project/info-prior/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Generalizing machine learning algorithms across medical centers is difficult. Data is often strongly biased towards each center, leading to different mappings from medical image X to segmented image Y.&lt;/p&gt;

&lt;p&gt;Instead of designing an adaptive classification model that would attempt to adjust its mapping X → Y for each center, I inform an unsupervised Bayesian segmentation model with how Y is supposed to look like. Specifically, I fit a smoothness prior on segmentations produced in one medical center and incorporate that as an informative empirical prior in a variational Bayesian image segmentation model. This model will produce segmentations in a target medical center that are as smooth as the segmentations produced in the source medical center.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The problem of domain adaptation in machine learning</title>
      <link>https://wmkouw.github.io/post/domain_adaptation/</link>
      <pubDate>Fri, 01 Feb 2019 12:19:17 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/post/domain_adaptation/</guid>
      <description>&lt;p&gt;Intelligent systems are deployed for a wide array of tasks, such as diagnosing patients, recognizing faces or translating texts &lt;a href=&#34;https://www.quantamagazine.org/the-rise-of-computer-aided-explanation-20150723/&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;. They are called &amp;ldquo;intelligent&amp;rdquo; because these machines are not specifically programmed to perform the task. Instead, they &lt;em&gt;learn&lt;/em&gt; to do their job from a &amp;ldquo;training set&amp;rdquo; of examples. For example, the system might be told to diagnose heart disease in patients based on biometrics such as age and cholesterol level. These biometrics vary across people and there is no fixed rule to form a diagnosis (e.g. &amp;ldquo;if older than 50 and cholesterol over 400, then patient is sick&amp;rdquo;). But, given enough examples of previous patients, the system can estimate the &lt;em&gt;risk&lt;/em&gt; of heart disease. The success of these types of systems has been stunning, mostly due to large-scale data collection efforts.&lt;/p&gt;

&lt;p&gt;The design of intelligent systems is based on statistical learning theory. A statistical classification model, or &lt;em&gt;classifier&lt;/em&gt;, will find a function to relate the biometrics in the examples to their diagnosis &lt;a href=&#34;https://arxiv.org/pdf/1710.09230.pdf&#34; target=&#34;_blank&#34;&gt;[2]&lt;/a&gt;. Each example, e.g. a healthy patient with a cholesterol level of 300, is a sample from an underlying probability distribution over possible levels of cholesterol in healthy or diseased patients. As the model sees more examples, it gets a better idea of what values are normal for healthy patients and what levels are to be expected for patients with heart disease. As such, it refines its function relating biometrics and diagnosis. We say that a model &lt;em&gt;generalizes&lt;/em&gt; when it can accurately classify new examples.&lt;/p&gt;

&lt;p&gt;Most of what is known on generalization is based on the premise that new examples are drawn from the same distribution as the training examples. However, that is not always the case. For example, suppose you measure age and cholesterol in two groups of patients at a particular hospital. One with and without cardiac problems. The figure below visualizes such a dataset with a scatterplot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wmkouw.github.io/img/hdis_budapest.png&#34; alt=&#34;Data from Budapest in Hungary&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You now fit a classifier and validate the model on new patients arriving to the hospital. The system works well and people are happy. Doctors from the US hear of your work and ask if they can use your model in their hospital. Sure, you say, and help them deploy it. However, it doesn&amp;rsquo;t work so well in the other hospital. What&amp;rsquo;s going on?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wmkouw.github.io/img/hdis_longbeach.png&#34; alt=&#34;Data from Budapest in Hungary&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As the scatterplot above shows, the patients from the other hospital are - on average - older. The system has been finely attuned to precise values of age and cholesterol, and will subsequently think that nearly all of the new patients are sick. The model is not designed to handle the shift in the data and will perform sub-optimally.&lt;/p&gt;

&lt;p&gt;These types of learning settings are called &lt;em&gt;domain&lt;/em&gt; &lt;em&gt;adaptation&lt;/em&gt; problems &lt;a href=&#34;https://pure.tudelft.nl/portal/files/48009162/kouw_techreport18a.pdf&#34; target=&#34;_blank&#34;&gt;[3]&lt;/a&gt;, where one hospital is the target &amp;ldquo;domain&amp;rdquo; and the other hospital is an additional source of information, called the &amp;ldquo;source domain&amp;rdquo;. The goal of a &amp;ldquo;domain-adaptive&amp;rdquo; classifier is to learn from the source domain and &amp;ldquo;adapt&amp;rdquo; to perform well in the target domain. As machine learning faces more complicated challenges, data shifts become more abundant and the need for adaptive models rises.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning an MR acquisition-invariant representation using Siamese neural networks</title>
      <link>https://wmkouw.github.io/publication/mrainet/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:02 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/mrainet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A review of single-source unsupervised domain adaptation</title>
      <link>https://wmkouw.github.io/publication/review/</link>
      <pubDate>Tue, 01 Jan 2019 16:13:59 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/review/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An introduction to domain adaptation and transfer learning</title>
      <link>https://wmkouw.github.io/publication/tr_introda/</link>
      <pubDate>Mon, 31 Dec 2018 14:46:06 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/tr_introda/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reducing variance in importance-weighted cross-validation under covariate shift</title>
      <link>https://wmkouw.github.io/publication/covshift-ctrlv/</link>
      <pubDate>Sun, 16 Dec 2018 06:34:24 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/covshift-ctrlv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CT image segmentation of bone for medical additive manufacturing using a CNN</title>
      <link>https://wmkouw.github.io/publication/bonectcnn/</link>
      <pubDate>Sat, 01 Dec 2018 09:44:47 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/bonectcnn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Big data &amp; trust</title>
      <link>https://wmkouw.github.io/talk/nsfworkshop-bigdata/</link>
      <pubDate>Sat, 08 Sep 2018 13:18:06 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/nsfworkshop-bigdata/</guid>
      <description>&lt;!-- # Data and trust --&gt;

&lt;p&gt;Research institutions, hospitals, governments and companies are collecting all kinds of data at an increased rate. Such “big data” offers the possibility to increase our understanding of human behavior, improve clinical diagnosis, and create intelligent consumer products. However, data can also end up in the hands of parties that may take advantage of you. For example, Facebook user data was leaked to political campaigners to influence voters [1]. Central to this debate is the concept of trust: how can you trust that the data that you give out will not end up hurting you?&lt;/p&gt;

&lt;p&gt;This debate was recently sparked by the adoption of the General Data Protection Regulation (GDPR), which gives individuals in the EU the right to review and destroy personal data collected by third parties. The GDPR greatly complicates the collection and dissemination of research data, particularly in medical sciences. How should researchers analyze data and publish their findings if their subjects decide that their data is to remain private? Can patient data be fully anonymized and yet remain traceable at the same time?  And how does data protection and privacy fit within the principle of open science that encourages researchers to make their data Findable, Accessible, Interoperable, and Reusable (FAIR)?&lt;/p&gt;

&lt;p&gt;An interesting novel technology in the data and trust debate is blockchain. The goal is to create a distributed transaction system without a central authority that profits from this position, for example the exchange of money without the intervention of a bank. Individual users can issue transactions that are subsequently verified by the entire community through a blockchain. If there’s foul play, for instance by someone changing their own balance, the transaction is rejected by the community. Blockchain thus ensures consensus between users that do not trust each other, which makes the technology interesting for a wide range of applications.&lt;/p&gt;

&lt;p&gt;In this workshop, we will present different views on how big data is currently being used. Furthermore, we will initiate an interactive discussion with the audience during which the participants can answer questions and share opinions using their smartphones.&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&#34;https://www.theguardian.com/news/2018/mar/17/cambridge-analytica-facebook-influence-us-election&#34; target=&#34;_blank&#34;&gt;https://www.theguardian.com/news/2018/mar/17/cambridge-analytica-facebook-influence-us-election&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Effects of sampling skewness of the importance-weighted risk estimator on model selection</title>
      <link>https://wmkouw.github.io/publication/covshift-skew/</link>
      <pubDate>Thu, 23 Aug 2018 13:01:29 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/covshift-skew/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effects of sampling skewness in importance-weighted cross-validation</title>
      <link>https://wmkouw.github.io/talk/icpr2018/</link>
      <pubDate>Mon, 20 Aug 2018 11:18:18 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/icpr2018/</guid>
      <description>&lt;p&gt;I presented my paper on how the importance-weighted risk estimator&amp;rsquo;s sampling distribution is skewed for small sample sizes, and how this affects hyperparameter selection during importance-weighted cross-validation under covariate shift.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Target contrastive pessimistic discriminant analysis</title>
      <link>https://wmkouw.github.io/publication/tcpr/</link>
      <pubDate>Thu, 21 Jun 2018 08:45:35 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/tcpr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>libtlda</title>
      <link>https://wmkouw.github.io/project/libtlda/</link>
      <pubDate>Sat, 09 Jun 2018 19:06:57 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/project/libtlda/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Libtlda is a library of classifiers designed for domain adaptation and transfer learning, available in Matlab and Python. It started out as the collection of classifiers that I implemented during my PhD, but is now being expanded with more methods and algorithms.&lt;/p&gt;

&lt;p&gt;Installation has been made easy, and there are demos to help you get started. Coders familiar with sci-kit will find it easy to pick up.&lt;/p&gt;

&lt;p&gt;More information is available on its documentation &lt;a href=&#34;https://libtlda.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34;&gt;page&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
