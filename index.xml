<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Researcher profile on Researcher profile</title>
    <link>https://wmkouw.github.io/</link>
    <description>Recent content in Researcher profile on Researcher profile</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Mon, 21 Dec 2020 17:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Target robust discriminant analysis</title>
      <link>https://wmkouw.github.io/publication/trda/</link>
      <pubDate>Mon, 21 Dec 2020 17:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/trda/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Variational Bayes for signal processing</title>
      <link>https://wmkouw.github.io/talk/sioux-seminar2020/</link>
      <pubDate>Thu, 05 Nov 2020 15:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/sioux-seminar2020/</guid>
      <description>&lt;p&gt;This is a talk for the Optimization / Machine Learning Competence group at Sioux Technologies. I presented the probabilistic derivations for the Kalman filter, variational Bayesian inference and some of BIASlab&amp;rsquo;s work on signal processing systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Online system identification using free energy minimisation</title>
      <link>https://wmkouw.github.io/talk/iwai2020/</link>
      <pubDate>Sun, 13 Sep 2020 09:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/iwai2020/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://wmkouw.github.io/posters/IWAI2020_wmkouw.png&#34; alt=&#34;Poster IWAI2020&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Online system identification in a Duffing oscillator by free energy minimisation</title>
      <link>https://wmkouw.github.io/publication/nsi-silverbox/</link>
      <pubDate>Tue, 01 Sep 2020 12:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/nsi-silverbox/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The data representativeness criterion</title>
      <link>https://wmkouw.github.io/publication/drc/</link>
      <pubDate>Tue, 11 Aug 2020 10:21:16 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/drc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Online variational message passing in hierarchical autoregressive models</title>
      <link>https://wmkouw.github.io/publication/vmphar/</link>
      <pubDate>Sun, 21 Jun 2020 12:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/vmphar/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AR-HGF</title>
      <link>https://wmkouw.github.io/project/ar-hgf/</link>
      <pubDate>Thu, 11 Jun 2020 13:26:41 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/project/ar-hgf/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;We address the problem of online Bayesian state and parameter tracking in autoregressive (AR) models with time-varying process noise variance. The involved marginalization and expectation integrals cannot be analytically solved. Moreover, the online tracking constraint makes sampling and batch learning methods unsuitable for this problem. We propose a hybrid variational message passing algorithm that robustly tracks the time-varying dynamics of the latent states, AR coefficients and process noise variance. Since message passing in a factor graph is a highly modular inference approach, the proposed methods easily extend to other non-stationary dynamic modeling problems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian joint state and parameter tracking in autoregressive models</title>
      <link>https://wmkouw.github.io/publication/vmpar-hgf/</link>
      <pubDate>Thu, 11 Jun 2020 12:00:00 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/vmpar-hgf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Schedule-free variational message passing for Bayesian filtering</title>
      <link>https://wmkouw.github.io/talk/neuromatch2020/</link>
      <pubDate>Tue, 31 Mar 2020 12:00:00 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/neuromatch2020/</guid>
      <description>&lt;p&gt;In Bayesian filtering, states and parameters of probabilistic state-space models are inferred in an online manner. Using the Free Energy Principle, the state-space model is cast to a generative model &lt;em&gt;p&lt;/em&gt; and the posterior distributions of interest are approximated using recognition distributions or beliefs &lt;em&gt;q&lt;/em&gt;. The factorisation of state-space models into state transitions and observation likelihoods over time supports forming a factor graph and performing inference via message passing.&lt;/p&gt;

&lt;p&gt;Tools for message passing on factor graphs typically employ a scheduling procedure, in which a separate algorithm or compiler takes the model description and returns &lt;em&gt;which&lt;/em&gt; nodes should pass messages &lt;em&gt;where&lt;/em&gt; at &lt;em&gt;what&lt;/em&gt; time. This can be sufficiently expensive to form a bottleneck. Moreover, it&amp;rsquo;s not a biologically plausible mechanism for governing message passing. I explore the possibility of passing messages without a scheduler. A designated terminal node should pass an initial message, which will arrive at an initial variable. The corresponding belief is updated, a local Free Energy is computed and the belief is emitted to neighbouring factor nodes. From there on out, whenever an updated belief arrives at a factor node, the node fires messages to all other variables if the local Free Energy surpasses a threshold. If not, the node becomes silent.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TUe 5SSD0 BMLIP</title>
      <link>https://wmkouw.github.io/teaching/tue-5ssd0/</link>
      <pubDate>Thu, 16 Jan 2020 13:26:41 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/teaching/tue-5ssd0/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://biaslab.github.io/teaching/bmlip/&#34; target=&#34;_blank&#34;&gt;Bayesian Machine Learning &amp;amp; Information Processing&lt;/a&gt; (BMLIP) is an elective course in the Electrical Engineering Master Program of the TU Eindhoven.&lt;/p&gt;

&lt;p&gt;BMLIP covers the fundamentals of the Bayesian (i.e., probabilistic) approach to machine learning and information processing systems. Firstly, we discuss many useful models including common regression and classification methods, Gaussian mixture models, hidden Markov models and Kalman filters. Secondly, we teach Expectation-Maximization (EM), Variational Bayes (VB), Variational Message Passing (VMP) and basic Monte Carlo sampling. Lastly, we discuss intelligent agents that learn purposeful behavior from interactions with their environment.&lt;/p&gt;

&lt;p&gt;I teach the mini-course on Probabilistic Programming, where we familiarize students with software for automatic inference in probabilistic models.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SSA-NLP</title>
      <link>https://wmkouw.github.io/project/ssa-nlp/</link>
      <pubDate>Fri, 15 Nov 2019 13:26:41 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/project/ssa-nlp/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Language evolves over time in many ways relevant to natural language processing tasks. For example, recent occurrences of tokens &amp;lsquo;BERT&amp;rsquo; and &amp;lsquo;ELMO&amp;rsquo; in publications refer to neural network architectures rather than persons [&lt;a href=&#34;https://www.aclweb.org/anthology/N19-1423/&#34; target=&#34;_blank&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1903.05987&#34; target=&#34;_blank&#34;&gt;2&lt;/a&gt;]. This type of temporal signal is typically overlooked, but is important if one aims to deploy a machine learning model over an extended period of time. In particular, language evolution causes data drift between time-steps in sequential decision-making tasks. Examples of such tasks include prediction of paper acceptance for yearly conferences (regular intervals) or author stance prediction for rumours on Twitter (irregular intervals). We tackle data drift by sequentially aligning learned representations. We argue that, due to its low computational expense, sequential alignment is a practical solution to dealing with language evolution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Back to the future - temporal adaptation of text representations</title>
      <link>https://wmkouw.github.io/publication/seqrum/</link>
      <pubDate>Mon, 11 Nov 2019 06:00:40 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/publication/seqrum/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust importance-weighted cross-validation under sample selection bias</title>
      <link>https://wmkouw.github.io/talk/mlsp2019/</link>
      <pubDate>Fri, 11 Oct 2019 09:11:43 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/talk/mlsp2019/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://wmkouw.github.io/posters/MLSP2019_wmkouw.png&#34; alt=&#34;Poster MLSP2019&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A review of domain adaptation without target labels</title>
      <link>https://wmkouw.github.io/publication/review/</link>
      <pubDate>Mon, 07 Oct 2019 16:13:59 +0200</pubDate>
      
      <guid>https://wmkouw.github.io/publication/review/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sequential domain-adaptive machine learning</title>
      <link>https://wmkouw.github.io/talk/nsf2019/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0100</pubDate>
      
      <guid>https://wmkouw.github.io/talk/nsf2019/</guid>
      <description>&lt;p&gt;This poster recaps two collaboration projects I did during my time as Niels Stensen Fellow at the University of Copenhagen. The main point of my proposal was to study &amp;ldquo;sequential domain adaptation&amp;rdquo;. The left column describes an application to &amp;ldquo;spatial sequences&amp;rdquo;, i.e., multi-site biomedical imaging, and the right column describes an application to &amp;ldquo;temporal sequences&amp;rdquo;, i.e. natural language processing over data collected in snapshots over time. The main take-home message from the work in this Fellowship is that domain adaptation is a solution to training machine learning models under sampling bias and that sequential adaptation allows for updating.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wmkouw.github.io/posters/NSF2019_wmkouw.png&#34; alt=&#34;Poster NSF2019&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
