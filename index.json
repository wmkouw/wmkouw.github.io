[{"authors":null,"categories":null,"content":"Research institutions, hospitals, governments and companies are collecting all kinds of data at an increased rate. Such “big data” offers the possibility to increase our understanding of human behavior, improve clinical diagnosis, and create intelligent consumer products. However, data can also end up in the hands of parties that may take advantage of you. For example, Facebook user data was leaked to political campaigners to influence voters [1]. Central to this debate is the concept of trust: how can you trust that the data that you give out will not end up hurting you?\nThis debate was recently sparked by the adoption of the General Data Protection Regulation (GDPR), which gives individuals in the EU the right to review and destroy personal data collected by third parties. The GDPR greatly complicates the collection and dissemination of research data, particularly in medical sciences. How should researchers analyze data and publish their findings if their subjects decide that their data is to remain private? Can patient data be fully anonymized and yet remain traceable at the same time? And how does data protection and privacy fit within the principle of open science that encourages researchers to make their data Findable, Accessible, Interoperable, and Reusable (FAIR)?\nAn interesting novel technology in the data and trust debate is blockchain. The goal is to create a distributed transaction system without a central authority that profits from this position, for example the exchange of money without the intervention of a bank. Individual users can issue transactions that are subsequently verified by the entire community through a blockchain. If there’s foul play, for instance by someone changing their own balance, the transaction is rejected by the community. Blockchain thus ensures consensus between users that do not trust each other, which makes the technology interesting for a wide range of applications.\nIn this workshop, we will present different views on how big data is currently being used. Furthermore, we will initiate an interactive discussion with the audience during which the participants can answer questions and share opinions using their smartphones.\n[1] https://www.theguardian.com/news/2018/mar/17/cambridge-analytica-facebook-influence-us-election\n","date":1536405486,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536405486,"objectID":"f54be9f9cacfa086dabea59e675e1470","permalink":"https://wmkouw.github.io/talk/nsfworkshop-bigdata/","publishdate":"2018-09-08T13:18:06+02:00","relpermalink":"/talk/nsfworkshop-bigdata/","section":"talk","summary":"Research institutions, hospitals, governments and companies are collecting all kinds of data at an increased rate. Such “big data” offers the possibility to increase our understanding of human behavior, improve clinical diagnosis, and create intelligent consumer products. However, data can also end up in the hands of parties that may take advantage of you. For example, Facebook user data was leaked to political campaigners to influence voters [1]. Central to this debate is the concept of trust: how can you trust that the data that you give out will not end up hurting you?","tags":["Data","Trust","GDPR"],"title":"Big Data \u0026 Trust","type":"talk"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1536404489,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536404489,"objectID":"bfb0bee66da7019e8150e1a9a7bbc281","permalink":"https://wmkouw.github.io/publication/covshift-skew/","publishdate":"2018-09-08T13:01:29+02:00","relpermalink":"/publication/covshift-skew/","section":"publication","summary":"Importance-weighting is a popular and well-researched technique for dealing with sample selection bias and covariate shift. It has desirable characteristics such as unbiasedness, consistency and low computational complexity. However, weighting can have a detrimental effect on an estimator as well. In this work, we empirically show that the sampling distribution of an importance-weighted estimator can be skewed. For sample selection bias settings, and for small sample sizes, the importance-weighted risk estimator produces overestimates for datasets in the body of the sampling distribution, i.e. the majority of cases, and large underestimates for data sets in the tail of the sampling distribution. These over- and underestimates of the risk lead to suboptimal regularization parameters when used for importance-weighted validation. ","tags":[],"title":"Effects of sampling skewness of the importance-weighted risk estimator on model selection","type":"publication"},{"authors":null,"categories":null,"content":"LibTLDA is a library of classifiers designed for domain adaptation and transfer learning, available in Matlab and Python. It started out as the collection of classifiers that I implemented during my PhD, but is now being expanded with more methods and algorithms.\nInstallation has been made easy, and there are demos to help you get started. Coders familiar with sci-kit will find it easy to employ.\nMore information is available on its github page and its documentation page.\n","date":1528564017,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528564017,"objectID":"0ab21211d334a19cc02646aaea399035","permalink":"https://wmkouw.github.io/project/libtlda/","publishdate":"2018-06-09T19:06:57+02:00","relpermalink":"/project/libtlda/","section":"project","summary":"Library of domain-adaptive classifiers and transfer learners.","tags":["machine learning","domain adaptation","transfer learning"],"title":"LibTLDA","type":"project"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1528461935,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528461935,"objectID":"6f99087f7dc10bea02297a292bc00b71","permalink":"https://wmkouw.github.io/publication/tcpr/","publishdate":"2018-06-08T14:45:35+02:00","relpermalink":"/publication/tcpr/","section":"publication","summary":"Domain-adaptive classifiers learn from a source domain and aim to generalize to a target domain. If the classifier's assumptions on the relationship between domains (e.g. covariate shift) are valid, then it will usually outperform a non-adaptive source classifier. Unfortunately, it can perform substantially worse when its assumptions are invalid. Validating these assumptions requires labeled target samples, which are usually not available. We argue that, in order to make domain-adaptive classifiers more practical, it is necessary to focus on robust methods; robust in the sense that the model still achieves a particular level of performance without making strong assumptions on the relationship between domains. With this objective in mind, we formulate a conservative parameter estimator that only deviates from the source classifier when a lower or equal risk is guaranteed for all possible labellings of the given target samples. We derive the corresponding estimator for a discriminant analysis model, and show that its risk is actually strictly smaller than that of the source classifier. Experiments indicate that our classifier outperforms state-of-the-art classifiers for geographically biased samples. ","tags":[],"title":"Target contrastive pessimistic discriminant analysis","type":"publication"},{"authors":null,"categories":null,"content":"NVPHBV is the Dutch Society for Pattern Recognition and Image Processing. During their meetings, researchers from the Netherlands have a chance to present some of their work and catch up on developments in their fields.\nI presented my work on removing MRI-scanner based varation from images.\n","date":1525887531,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525887531,"objectID":"9ea9a9a81abfe8ad41df86e23ae80ce8","permalink":"https://wmkouw.github.io/talk/nvphbv2018/","publishdate":"2018-05-09T19:38:51+02:00","relpermalink":"/talk/nvphbv2018/","section":"talk","summary":"NVPHBV is the Dutch Society for Pattern Recognition and Image Processing. During their meetings, researchers from the Netherlands have a chance to present some of their work and catch up on developments in their fields.\nI presented my work on removing MRI-scanner based varation from images.","tags":["machine learning","domain adaptation","robust estimator"],"title":"MR Acquisition-invariant representation learning","type":"talk"},{"authors":null,"categories":null,"content":"For my second project at the Netherlands eScience Center, I worked on a collaboration project between Susan Hogervorst at the Open University, Marieke van Erp at the Digital Humanities Lab of the KNAW, and Hennie Brugman of the Meertens Institute, along with my partner engineer Martine de Vos.\nMuch of our historical knowledge is based on oral or written accounts of eyewitnesses, particularly in cases of war and mass violence, when regular ways of documentation and record keeping are often absent. Although oral history and the study of ego documents both value these individual perspectives on history and its meaning, these research fields tend to operate separately. However, the digital revolution has shaken up the balance between spoken and written text. The paradigm emerging in the application of search technology to digitised oral history is characterised by a post-documentary sensibility: away from text and sensitive to other dimensions of human expression than language. Nonetheless, ‘mining’ of oral history accounts remains valuable in humanities research, especially considering the re-use of digital interview collections throughout the humanities.\nEviDENce explores new ways of analysing and contextualising historical sources by applying event modelling and semantic web technologies. Our project suggests a systematic and integral content analysis of ‘ego-sources’ by applying state-of-the-art entity and event modelling methods and tools, in order to explore the nature and value of ego-sources and to disclose existing collections. We focus on representations of mass-violence in a case study to generate and explore different kinds of events: a synchronic analysis of WW2 events, centered around the oral history collection ‘Getuigenverhalen’.\nOur first steps were to develop a container for the natural language processing pipeline known as NewsReader . This allowed us to process the transcribed oral history interviews from \u0026lsquo;Getuigenverhalen\u0026rsquo;. Further work is ongoing.\nLinks to the eScience project page and the Github organization.\n","date":1507569104,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507569104,"objectID":"6266ddf37910139b4412d01f5c23cdd8","permalink":"https://wmkouw.github.io/project/evidence/","publishdate":"2017-10-09T19:11:44+02:00","relpermalink":"/project/evidence/","section":"project","summary":"Violent event detection in transcribed oral history interviews.","tags":["natural language processing","historical research"],"title":"EviDENce","type":"project"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1506861944,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506861944,"objectID":"174a6a52695cb967b02b002d8e21095c","permalink":"https://wmkouw.github.io/publication/covshift-ctrl/","publishdate":"2017-10-01T14:45:44+02:00","relpermalink":"/publication/covshift-ctrl/","section":"publication","summary":"Covariate shift classification problems can in principle be tackled by importance-weighting training samples. However, the sampling variance of the risk estimator is often scaled up dramatically by the weights. This means that during cross-validation - when the importance-weighted risk is repeatedly evaluated - suboptimal hyperparameter estimates are produced. We study the sampling variances of the importance-weighted versus the oracle estimator as a function of the relative scale of the training data. We show that introducing a control variate can reduce the variance of the importance-weighted risk estimator, which leads to superior regularization parameter estimates when the training data is much smaller in scale than the test data.","tags":[],"title":"Reducing sampling variance in covariate shift using control variates","type":"publication"},{"authors":null,"categories":null,"content":"My first project at the Netherlands eScience Center was with the winner of the Young eScientist Award 2016, dr. Maureen van Eijnatten. She worked at the 3D Innovation Lab under the department of Maxillo-Facial Surgery at the Vrije Universiteit medical center (VUmc), together with PhD student Jordi Minnema and medical engineer Niels Liberton.\nDuring the project, we developed a method for automatically detecting bone tissue in medical CT-scanner images. This method consisted of an artificial neural network, called a convolutional neural network (CNN), which looks at individual voxels in the image plus their surroundings. If a voxel shows an intensity value suitable for bone tissue and is surrounded voxels with similar intensity values (noisy signal voxels are often isolated), then the network classifies that voxel as bone. It performs its function for a whole CT-scan, which allows us to automatically reconstruct a 3D model of the skull. This 3D model is later used to design and 3D-print saw templates and other constructs to aid surgeons.\nLink to the code and the eScience Center page.\n","date":1504977174,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504977174,"objectID":"c43a9b8e6c0d6ff24287516d1a140c59","permalink":"https://wmkouw.github.io/project/mfs-bonecnn/","publishdate":"2017-09-09T19:12:54+02:00","relpermalink":"/project/mfs-bonecnn/","section":"project","summary":"Bone tissue recognition from CT-scans of maxillo-facial surgery patients.","tags":["deep learning","medical imaging","maxillo-facial surgery"],"title":"MFS-boneCNN","type":"project"},{"authors":["W.M. Kouw","M. Loog","W. Bartels","A.M. Mendrik"],"categories":null,"content":"","date":1504874325,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504874325,"objectID":"515350e69b50ecab6835070f81b623a8","permalink":"https://wmkouw.github.io/publication/mrainet/","publishdate":"2017-09-08T14:38:45+02:00","relpermalink":"/publication/mrainet/","section":"publication","summary":"Voxelwise classification approaches are popular and effective methods for tissue quantification in brain magnetic resonance imaging (MRI) scans. However, generalization of these approaches is hampered by large differences between sets of MRI scans such as differences in field strength, vendor or acquisition protocols. Due to this acquisition related variation, classifiers trained on data from a specific scanner fail or under-perform when applied to data that was acquired differently. In order to address this lack of generalization, we propose a Siamese neural network (MRAI-net) to learn a representation that minimizes the between-scanner variation, while maintaining the contrast between brain tissues necessary for brain tissue quantification. The proposed MRAI-net was evaluated on both simulated and real MRI data. After learning the MR acquisition invariant representation, any supervised classification model that uses feature vectors can be applied. In this paper, we provide a proof of principle, which shows that a linear classifier applied on the MRAI representation is able to outperform supervised convolutional neural network classifiers for tissue classification when little target training data is available. ","tags":[],"title":"MR Acquisition-invariant feature extraction using Siamese neural networks.","type":"publication"},{"authors":null,"categories":null,"content":"ICT.OPEN is the Dutch national conference for computer science in general. Researchers from various universities and companies present their work along different tracks.\nI presented my work in the Machine Learning track. My topic was cross-validation under covariate shift and how to reduce variance in the importance-weighted risk estimator.\n","date":1489079804,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489079804,"objectID":"3b58fc47ee57e9092bf91cdd84b9eb61","permalink":"https://wmkouw.github.io/talk/ictopen2017/","publishdate":"2017-03-09T19:16:44+02:00","relpermalink":"/talk/ictopen2017/","section":"talk","summary":"ICT.OPEN is the Dutch national conference for computer science in general. Researchers from various universities and companies present their work along different tracks.\nI presented my work in the Machine Learning track. My topic was cross-validation under covariate shift and how to reduce variance in the importance-weighted risk estimator.","tags":["machine learning","covariate shift","importance-weighting","variance reduction"],"title":"Variance reduction techniques for importance-weighted cross-validation","type":"talk"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1480588890,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480588890,"objectID":"fd21f3090c6ff551eade84e32ba85f5f","permalink":"https://wmkouw.github.io/publication/covshift-l2reg/","publishdate":"2016-12-01T12:41:30+02:00","relpermalink":"/publication/covshift-l2reg/","section":"publication","summary":"This paper identifies a problem with the usual procedure for L2-regularization parameter estimation in a domain adaptation setting. In such a setting, there are differences between the distributions generating the training data (source domain) and the test data (target domain). The usual cross-validation procedure requires validation data, which can not be obtained from the unlabeled target data. The problem is that if one decides to use source validation data, the regularization parameter is underestimated. One possible solution is to scale the source validation data through importance weighting, but we show that this correction is not sufficient. We conclude the paper with an empirical analysis of the effect of several importance weight estimators on the estimation of the regularization parameter.","tags":[],"title":"On regularization parameter estimation under covariate shift","type":"publication"},{"authors":["W.M. Kouw","L.J.P. van der Maaten","J.H. Krijthe","M. Loog"],"categories":null,"content":"","date":1477996402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477996402,"objectID":"884d96bfc381538193f86239ebeabb5f","permalink":"https://wmkouw.github.io/publication/flda/","publishdate":"2016-11-01T12:33:22+02:00","relpermalink":"/publication/flda/","section":"publication","summary":"Domain adaptation is the supervised learning setting in which the training and test data are sampled from different distributions: training data is sampled from a source domain, whilst test data is sampled from a target domain. This paper proposes and studies an approach, called feature-level domain adaptation (FLDA), that models the dependence between the two domains by means of a feature-level transfer model that is trained to describe the transfer from source to target domain. Subsequently, we train a domain-adapted classifier by minimizing the expected loss under the resulting transfer model. For linear classifiers and a large family of loss functions and transfer models, this expected loss can be computed or approximated analytically, and minimized efficiently. Our empirical evaluation of FLDA focuses on problems comprising binary and count data in which the transfer can be naturally modeled via a dropout distribution, which allows the classifier to adapt to differences in the marginal probability of features in the source and the target domain. Our experiments on several real- world problems show that FLDA performs on par with state- of- the-art domain-adaptation techniques.","tags":[],"title":"Feature-level domain adaptation","type":"publication"},{"authors":null,"categories":null,"content":"NVPHBV is the Dutch Society for Pattern Recognition and Image Processing. During their meetings, researchers from the Netherlands have a chance to present some of their work and catch up on developments in their fields.\nI presented my work on a robust estimator for linear discriminant analysis in domain-adaptive machine learning.\n","date":1464369513,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464369513,"objectID":"ab583c637ab232eef2cef0fe8e38ea0f","permalink":"https://wmkouw.github.io/talk/nvphbv2016/","publishdate":"2016-05-27T19:18:33+02:00","relpermalink":"/talk/nvphbv2016/","section":"talk","summary":"NVPHBV is the Dutch Society for Pattern Recognition and Image Processing. During their meetings, researchers from the Netherlands have a chance to present some of their work and catch up on developments in their fields.\nI presented my work on a robust estimator for linear discriminant analysis in domain-adaptive machine learning.","tags":["machine learning","domain adaptation","robust estimator"],"title":"Target Contrastive Estimator for Robust Domain Adaptation","type":"talk"}]