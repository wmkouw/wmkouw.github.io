[{"authors":["Wouter M. Kouw"],"categories":[],"content":"Intelligent systems are deployed for a wide array of tasks, such as diagnosing patients, recognizing faces or translating texts [1]. They are called \u0026ldquo;intelligent\u0026rdquo; because these machines are not specifically programmed to perform the task. Instead, they learn to do their job from a \u0026ldquo;training set\u0026rdquo; of examples. For example, the system might be told to diagnose heart disease in patients based on biometrics such as age and cholesterol level. These biometrics vary across people and there is no fixed rule to form a diagnosis (e.g. \u0026ldquo;if older than 50 and cholesterol over 400, then patient is sick\u0026rdquo;). But, given enough examples of previous patients, the system can estimate the risk of heart disease. The success of these types of systems has been stunning, mostly due to large-scale data collection efforts.\nThe design of intelligent systems is based on statistical learning theory. A statistical classification model, or classifier, will find a function to relate the biometrics in the examples to their diagnosis [2]. Each example, e.g. a healthy patient with a cholesterol level of 300, is a sample from an underlying probability distribution over possible levels of cholesterol in healthy or diseased patients. As the model sees more examples, it gets a better idea of what values are normal for healthy patients and what levels are to be expected for patients with heart disease. As such, it refines its function relating biometrics and diagnosis. We say that a model generalizes when it can accurately classify new examples.\nMost of what is known on generalization is based on the premise that new examples are drawn from the same distribution as the training examples. However, that is not always the case. For example, suppose you measure age and cholesterol in two groups of patients at a particular hospital. One with and without cardiac problems. The figure below visualizes such a dataset with a scatterplot:\nYou now fit a classifier and validate the model on new patients arriving to the hospital. The system works well and people are happy. Doctors from the US hear of your work and ask if they can use your model in their hospital. Sure, you say, and help them deploy it. However, it doesn\u0026rsquo;t work so well in the other hospital. What\u0026rsquo;s going on?\nAs the scatterplot above shows, the patients from the other hospital are - on average - older. The system has been finely attuned to precise values of age and cholesterol, and will subsequently think that nearly all of the new patients are sick. The model is not designed to handle the shift in the data and will perform sub-optimally.\nThese types of learning settings are called domain adaptation problems [3], where one hospital is the target \u0026ldquo;domain\u0026rdquo; and the other hospital is an additional source of information, called the \u0026ldquo;source domain\u0026rdquo;. The goal of a \u0026ldquo;domain-adaptive\u0026rdquo; classifier is to learn from the source domain and \u0026ldquo;adapt\u0026rdquo; to perform well in the target domain. As machine learning faces more complicated challenges, data shifts become more abundant and the need for adaptive models rises.\n","date":1549019957,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549019957,"objectID":"03708e666e25ee585d4fdfd2c0bed8a5","permalink":"https://wmkouw.github.io/post/domain_adaptation/","publishdate":"2019-02-01T12:19:17+01:00","relpermalink":"/post/domain_adaptation/","section":"post","summary":"Intelligent systems are deployed for a wide array of tasks, such as diagnosing patients, recognizing faces or translating texts [1]. They are called \u0026ldquo;intelligent\u0026rdquo; because these machines are not specifically programmed to perform the task. Instead, they learn to do their job from a \u0026ldquo;training set\u0026rdquo; of examples. For example, the system might be told to diagnose heart disease in patients based on biometrics such as age and cholesterol level. These biometrics vary across people and there is no fixed rule to form a diagnosis (e.","tags":["machine-learning","pattern-recognition","domain-adaptation"],"title":"The problem of domain adaptation in machine learning","type":"post"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1547648039,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547648039,"objectID":"1895a75cd101c0cd929281f5f1e5d39e","permalink":"https://wmkouw.github.io/publication/review/","publishdate":"2019-01-16T16:13:59+02:00","relpermalink":"/publication/review/","section":"publication","summary":"Domain adaptation has become a prominent problem setting in machine learning and related fields. This review asks the questions: when and how a classifier can learn from a source domain and generalize to a target domain. As for when, we review conditions that allow for cross-domain generalization error bounds. As for how, we present a categorization of approaches, divided into, what we refer to as, sample-based, feature-based and inference-based methods. Sample-based methods focus on weighting individual observations during training based on their importance to the target domain. Feature-based methods focus on mapping, projecting and representing features such that a source classifier performs well on the target domain and inference-based methods focus on alternative estimators, such as robust, minimax or Bayesian. Our categorization highlights recurring ideas and raises a number of questions important to further research.","tags":["machine-learning","domain-adaptation","transfer-learning","covariate-shift","sample-selection-bias"],"title":"A review of single-source unsupervised domain adaptation","type":"publication"},{"authors":["W.M. Kouw, M. Loog, L.W. Bartels, A.M. Mendrik"],"categories":null,"content":"","date":1547589602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547589602,"objectID":"e088a0336afc427926c5a3fa27caa914","permalink":"https://wmkouw.github.io/publication/mrainet_small/","publishdate":"2019-01-16T00:00:02+02:00","relpermalink":"/publication/mrainet_small/","section":"publication","summary":"Generalization of voxelwise classifiers is hampered by differences between MRI-scanners, e.g. different acquisition protocols and field strengths. To address this limitation, we propose a Siamese neural network (MRAI-NET) that extracts acquisition-invariant feature vectors. These can consequently be used by task-specific methods, such as voxelwise classifiers for tissue segmentation. MRAI-NET is tested on both simulated and real patient data. Experiments show that MRAI-NET outperforms voxelwise classifiers trained on the source or target scanner data when a small number of labeled samples is available.","tags":[],"title":"Learning an MR acquisition-invariant representation using Siamese neural networks","type":"publication"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1546263966,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546263966,"objectID":"30d612eead3dfdfa611ac598569d8dc1","permalink":"https://wmkouw.github.io/publication/tr_introda/","publishdate":"2018-12-31T14:46:06+01:00","relpermalink":"/publication/tr_introda/","section":"publication","summary":"In machine learning, if the training data is an unbiased sample of an underlying distribution, then the learned classification function will make accurate predictions for new samples. However, if the training data is _not_ an unbiased sample, then there will be differences between how the training data is distributed and how the test data is distributed. Standard classifiers cannot cope with changes in data distributions between training and test phases, and will not perform well. _Domain_ _adaptation_ and _transfer_ _learning_ are sub-fields within machine learning that are concerned with accounting for these types of changes. Here, I present an introduction to these fields, guided by the question: when and how can a classifier generalize from a source to a target domain? I will start with a brief introduction into risk minimization, and how transfer learning and domain adaptation expand upon this framework. Following that, I discuss three common simple data set shifts, namely prior, covariate and concept shift. For more complex domain shifts, there are a wide variety of approaches. These are categorized into: importance-weighting, subspace mapping, domain-invariant projections, feature augmentation, minimax estimators and robust algorithms. A number of points will arise, which I will discuss in the last section. I conclude with the remark that many open questions will have to be addressed before transfer learners and domain-adaptive classifiers become practical.","tags":[],"title":"An introduction to domain adaptation and transfer learning","type":"publication"},{"authors":["W.M. Kouw","J.H. Krijthe","M. Loog"],"categories":null,"content":"","date":1546061664,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546061664,"objectID":"e9073895652951f8bfdc4add0fbf3d76","permalink":"https://wmkouw.github.io/publication/covshift-ctrlv/","publishdate":"2018-12-29T06:34:24+01:00","relpermalink":"/publication/covshift-ctrlv/","section":"publication","summary":"Covariate shift classification problems can in principle be tackled by importance-weighting of training samples. However, the sampling variance of the risk estimator is often scaled up dramatically by employing such weighting. One of the consequences of this is that during cross-validation – when the importance-weighted risk is repeatedly estimated – suboptimal hyperparameter estimates are produced. We study the sampling variance of the importance-weighted risk estimator as a function of the width of the source distribution. We show that introducing a control variate can reduce its sampling variance, which leads to improved regularization parameter estimates when the training data is smaller in scale than the test data.","tags":[],"title":"Reducing variance in importance-weighted cross-validation under covariate shift","type":"publication"},{"authors":["J. Minnema","M. van Eijnatten","W.M. Kouw","F. Diblen","A.M. Mendrik","J. Wolff"],"categories":null,"content":"","date":1543650287,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543650287,"objectID":"9e3dd1b900be14bb402cdfb99f141760","permalink":"https://wmkouw.github.io/publication/bonectcnn/","publishdate":"2018-12-01T09:44:47+02:00","relpermalink":"/publication/bonectcnn/","section":"publication","summary":"OBJECTIVES: The most tedious and time-consuming task in medical additive manufacturing (AM) is image segmentation. The aim of the present study was to develop and train a convolutional neural network (CNN) for bone segmentation in computed tomography (CT) scans. METHODS: The CNN was trained with CT scans acquired using six different scanners. Standard tessellation language (STL) models of 20 patients who had previously undergone craniotomy and cranioplasty using additively manufactured skull implants served as “gold standard” models during CNN training. The CNN segmented all patient CT scans using a leave-2-out cross-validation scheme. All segmented CT scans were converted into STL models and geometrically compared with the gold standard STL models. RESULTS: The CT scans segmented using the CNN demonstrated a large overlap with the gold standard segmentation and resulted in a mean Dice similarity coefficient of 0.92 ± 0.04. The CNN-based STL models demonstrated mean surface deviations ranging between -0.19 mm ± 0.86 mm and 1.22 mm ± 1.75 mm, when compared to the gold standard STL models. No major differences were observed between the mean deviations of the CNN-based STL models acquired using six different CT scanners. CONCLUSIONS: The fully-automated CNN was able to accurately segment the skull. CNNs thus offer the opportunity of removing the current prohibitive barriers of time and effort during CT image segmentation, making patient-specific AM constructs more accesible.","tags":["medical-imaging","computed-tomography","deep-learning","additive manufacturing"],"title":"CT image segmentation of bone for medical additive manufacturing using a CNN","type":"publication"},{"authors":null,"categories":null,"content":"Research institutions, hospitals, governments and companies are collecting all kinds of data at an increased rate. Such “big data” offers the possibility to increase our understanding of human behavior, improve clinical diagnosis, and create intelligent consumer products. However, data can also end up in the hands of parties that may take advantage of you. For example, Facebook user data was leaked to political campaigners to influence voters [1]. Central to this debate is the concept of trust: how can you trust that the data that you give out will not end up hurting you?\nThis debate was recently sparked by the adoption of the General Data Protection Regulation (GDPR), which gives individuals in the EU the right to review and destroy personal data collected by third parties. The GDPR greatly complicates the collection and dissemination of research data, particularly in medical sciences. How should researchers analyze data and publish their findings if their subjects decide that their data is to remain private? Can patient data be fully anonymized and yet remain traceable at the same time? And how does data protection and privacy fit within the principle of open science that encourages researchers to make their data Findable, Accessible, Interoperable, and Reusable (FAIR)?\nAn interesting novel technology in the data and trust debate is blockchain. The goal is to create a distributed transaction system without a central authority that profits from this position, for example the exchange of money without the intervention of a bank. Individual users can issue transactions that are subsequently verified by the entire community through a blockchain. If there’s foul play, for instance by someone changing their own balance, the transaction is rejected by the community. Blockchain thus ensures consensus between users that do not trust each other, which makes the technology interesting for a wide range of applications.\nIn this workshop, we will present different views on how big data is currently being used. Furthermore, we will initiate an interactive discussion with the audience during which the participants can answer questions and share opinions using their smartphones.\n[1] https://www.theguardian.com/news/2018/mar/17/cambridge-analytica-facebook-influence-us-election\n","date":1536405486,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536405486,"objectID":"f54be9f9cacfa086dabea59e675e1470","permalink":"https://wmkouw.github.io/talk/nsfworkshop-bigdata/","publishdate":"2018-09-08T13:18:06+02:00","relpermalink":"/talk/nsfworkshop-bigdata/","section":"talk","summary":"Research institutions, hospitals, governments and companies are collecting all kinds of data at an increased rate. Such “big data” offers the possibility to increase our understanding of human behavior, improve clinical diagnosis, and create intelligent consumer products. However, data can also end up in the hands of parties that may take advantage of you. For example, Facebook user data was leaked to political campaigners to influence voters [1]. Central to this debate is the concept of trust: how can you trust that the data that you give out will not end up hurting you?","tags":["Data","trust","GDPR"],"title":"Big data \u0026 trust","type":"talk"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1535022089,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535022089,"objectID":"bfb0bee66da7019e8150e1a9a7bbc281","permalink":"https://wmkouw.github.io/publication/covshift-skew/","publishdate":"2018-08-23T13:01:29+02:00","relpermalink":"/publication/covshift-skew/","section":"publication","summary":"Importance-weighting is a popular and well-researched technique for dealing with sample selection bias and covariate shift. It has desirable characteristics such as unbiasedness, consistency and low computational complexity. However, weighting can have a detrimental effect on an estimator as well. In this work, we empirically show that the sampling distribution of an importance-weighted estimator can be skewed. For sample selection bias settings, and for small sample sizes, the importance-weighted risk estimator produces overestimates for datasets in the body of the sampling distribution, i.e. the majority of cases, and large underestimates for data sets in the tail of the sampling distribution. These over- and underestimates of the risk lead to suboptimal regularization parameters when used for importance-weighted validation. ","tags":["covariate-shift","cross-validation","sampling-skewness"],"title":"Effects of sampling skewness of the importance-weighted risk estimator on model selection","type":"publication"},{"authors":null,"categories":null,"content":"I presented my paper on how the importance-weighted risk estimator\u0026rsquo;s sampling distribution is skewed for small sample sizes, and how this affects hyperparameter selection during importance-weighted cross-validation under covariate shift.\n","date":1534756698,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534756698,"objectID":"d493ea0a6310fd4f92493d5d90897d46","permalink":"https://wmkouw.github.io/talk/icpr2018/","publishdate":"2018-08-20T11:18:18+02:00","relpermalink":"/talk/icpr2018/","section":"talk","summary":"I presented my paper on how the importance-weighted risk estimator\u0026rsquo;s sampling distribution is skewed for small sample sizes, and how this affects hyperparameter selection during importance-weighted cross-validation under covariate shift.","tags":["importance-weighted-risk","cross-validation","covariate-shift","sampling-skewness"],"title":"Effects of sampling skewness in importance-weighted cross-validation","type":"talk"},{"authors":null,"categories":null,"content":"\nLibTLDA is a library of classifiers designed for domain adaptation and transfer learning, available in Matlab and Python. It started out as the collection of classifiers that I implemented during my PhD, but is now being expanded with more methods and algorithms.\nInstallation has been made easy, and there are demos to help you get started. Coders familiar with sci-kit will find it easy to pick up.\nMore information is available on its documentation page.\n","date":1528564017,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528564017,"objectID":"0ab21211d334a19cc02646aaea399035","permalink":"https://wmkouw.github.io/project/libtlda/","publishdate":"2018-06-09T19:06:57+02:00","relpermalink":"/project/libtlda/","section":"project","summary":"Library of domain-adaptive classifiers and transfer learners.","tags":["machine-learning","domain-adaptation","transfer-learning"],"title":"LibTLDA","type":"project"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1528526735,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528526735,"objectID":"6f99087f7dc10bea02297a292bc00b71","permalink":"https://wmkouw.github.io/publication/tcpr/","publishdate":"2018-06-09T08:45:35+02:00","relpermalink":"/publication/tcpr/","section":"publication","summary":"Domain-adaptive classifiers learn from a source domain and aim to generalize to a target domain. If the classifier's assumptions on the relationship between domains (e.g. covariate shift) are valid, then it will usually outperform a non-adaptive source classifier. Unfortunately, it can perform substantially worse when its assumptions are invalid. Validating these assumptions requires labeled target samples, which are usually not available. We argue that, in order to make domain-adaptive classifiers more practical, it is necessary to focus on robust methods; robust in the sense that the model still achieves a particular level of performance without making strong assumptions on the relationship between domains. With this objective in mind, we formulate a conservative parameter estimator that only deviates from the source classifier when a lower or equal risk is guaranteed for all possible labellings of the given target samples. We derive the corresponding estimator for a discriminant analysis model, and show that its risk is actually strictly smaller than that of the source classifier. Experiments indicate that our classifier outperforms state-of-the-art classifiers for geographically biased samples. ","tags":["domain-adaptation","estimator","robustness"],"title":"Target contrastive pessimistic discriminant analysis","type":"publication"},{"authors":null,"categories":null,"content":"NVPHBV is the Dutch Society for Pattern Recognition and Image Processing. During their meetings, researchers from the Netherlands have a chance to present some of their work and catch up on developments in their fields.\nI presented my work on removing MRI-scanner based varation from images.\n","date":1525887531,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525887531,"objectID":"9ea9a9a81abfe8ad41df86e23ae80ce8","permalink":"https://wmkouw.github.io/talk/nvphbv2018/","publishdate":"2018-05-09T19:38:51+02:00","relpermalink":"/talk/nvphbv2018/","section":"talk","summary":"NVPHBV is the Dutch Society for Pattern Recognition and Image Processing. During their meetings, researchers from the Netherlands have a chance to present some of their work and catch up on developments in their fields.\nI presented my work on removing MRI-scanner based varation from images.","tags":["representation-learning","feature-extraction","MRI"],"title":"MR acquisition-invariant representation learning","type":"talk"},{"authors":null,"categories":null,"content":"\nWhile I was at the Netherlands eScience Center, I worked on the Young eScientist Award 2016 project, together with its winner Maureen van Eijnatten and PhD student Jordi Minnema. They\u0026rsquo;re part of the 3D Innovation Lab under the department of Maxillo-Facial Surgery at the Vrije Universiteit medical center.\nDuring the project, we developed a method for automatically detecting bone tissue in medical CT-scanner images. This method consisted of an artificial neural network, called a convolutional neural network (CNN), which looks at individual voxels in the image plus their surroundings. If a voxel shows an intensity value suitable for bone tissue and is surrounded voxels with similar intensity values (noisy signal voxels are often isolated), then the network classifies that voxel as bone. It performs its function for a whole CT-scan, which allows us to automatically reconstruct a 3D model of the skull. This 3D model is later used to design and 3D-print saw templates and other constructs to aid surgeons.\n","date":1504977174,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504977174,"objectID":"c43a9b8e6c0d6ff24287516d1a140c59","permalink":"https://wmkouw.github.io/project/mfs-bonecnn/","publishdate":"2017-09-09T19:12:54+02:00","relpermalink":"/project/mfs-bonecnn/","section":"project","summary":"Bone tissue recognition from CT-scans of maxillo-facial surgery patients.","tags":["deep-learning","medical-imaging","maxillo-facial-surgery"],"title":"YEAP16","type":"project"},{"authors":null,"categories":null,"content":"\nSuppose you have two sets of MR images, that were acquired using different scanners and/or different scanning protocols. One set of images is brighter and shows more tissue contrast than the other. One of them has annotations and the other does not. Tissue classifiers \u0026ndash; important to computer-aided diagnosis systems \u0026ndash; can be trained on the annotated data from one scanner, but when applied to data from the other scanner they will drastically under-perform. It is exactly what makes computer systems so powerful, i.e. looking at minute variations in pixel intensities, that also makes them vulnerable to data set shifts.\nWe developed a method called MR Acquisition-Invariant Neural Network that aims to learn a representation of patches such that these show minimal variation with respect to the MRI scanner without losing clinically-relevant tissue variation. Once trained, the network extracts acquisition-invariant feature vectors, which can be used for a variety of tasks in medical images later on.\nMore information is available on its documentation page.\n","date":1496333089,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496333089,"objectID":"60b752f8e01c22e789347b81fe65961e","permalink":"https://wmkouw.github.io/project/mrainet/","publishdate":"2017-06-01T18:04:49+02:00","relpermalink":"/project/mrainet/","section":"project","summary":"MR acquisition-invarant representation learning using Siamese neural networks.","tags":["MRI","acquisition-variation","representation-learning"],"title":"MRAI-net","type":"project"},{"authors":null,"categories":null,"content":"ICT.OPEN is the Dutch national conference for computer science in general. Researchers from various universities and companies present their work along different tracks.\nI presented my work in the Machine Learning track. My topic was cross-validation under covariate shift and how to reduce variance in the importance-weighted risk estimator.\n","date":1489079804,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489079804,"objectID":"3b58fc47ee57e9092bf91cdd84b9eb61","permalink":"https://wmkouw.github.io/talk/ictopen2017/","publishdate":"2017-03-09T19:16:44+02:00","relpermalink":"/talk/ictopen2017/","section":"talk","summary":"ICT.OPEN is the Dutch national conference for computer science in general. Researchers from various universities and companies present their work along different tracks.\nI presented my work in the Machine Learning track. My topic was cross-validation under covariate shift and how to reduce variance in the importance-weighted risk estimator.","tags":["machine-learning","covariate-shift","importance-weighting","variance-reduction"],"title":"Variance reduction techniques for importance-weighted cross-validation","type":"talk"},{"authors":null,"categories":null,"content":"I presented my paper on problems with importance-weighted cross-validation under covariate shift.\n","date":1481361495,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481361495,"objectID":"ce9cc4f9daa9d554df50e4877c5d0b65","permalink":"https://wmkouw.github.io/talk/icpr2016/","publishdate":"2016-12-10T11:18:15+02:00","relpermalink":"/talk/icpr2016/","section":"talk","summary":"I presented my paper on problems with importance-weighted cross-validation under covariate shift.","tags":["cross-validation","covariate-shift"],"title":"On cross-validation under covariate shift","type":"talk"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1480588890,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480588890,"objectID":"fd21f3090c6ff551eade84e32ba85f5f","permalink":"https://wmkouw.github.io/publication/covshift-l2reg/","publishdate":"2016-12-01T12:41:30+02:00","relpermalink":"/publication/covshift-l2reg/","section":"publication","summary":"This paper identifies a problem with the usual procedure for L2-regularization parameter estimation in a domain adaptation setting. In such a setting, there are differences between the distributions generating the training data (source domain) and the test data (target domain). The usual cross-validation procedure requires validation data, which can not be obtained from the unlabeled target data. The problem is that if one decides to use source validation data, the regularization parameter is underestimated. One possible solution is to scale the source validation data through importance weighting, but we show that this correction is not sufficient. We conclude the paper with an empirical analysis of the effect of several importance weight estimators on the estimation of the regularization parameter.","tags":[],"title":"On regularization parameter estimation under covariate shift","type":"publication"},{"authors":["W.M. Kouw","L.J.P. van der Maaten","J.H. Krijthe","M. Loog"],"categories":null,"content":"","date":1477996402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477996402,"objectID":"884d96bfc381538193f86239ebeabb5f","permalink":"https://wmkouw.github.io/publication/flda/","publishdate":"2016-11-01T12:33:22+02:00","relpermalink":"/publication/flda/","section":"publication","summary":"Domain adaptation is the supervised learning setting in which the training and test data are sampled from different distributions: training data is sampled from a source domain, whilst test data is sampled from a target domain. This paper proposes and studies an approach, called feature-level domain adaptation (FLDA), that models the dependence between the two domains by means of a feature-level transfer model that is trained to describe the transfer from source to target domain. Subsequently, we train a domain-adapted classifier by minimizing the expected loss under the resulting transfer model. For linear classifiers and a large family of loss functions and transfer models, this expected loss can be computed or approximated analytically, and minimized efficiently. Our empirical evaluation of FLDA focuses on problems comprising binary and count data in which the transfer can be naturally modeled via a dropout distribution, which allows the classifier to adapt to differences in the marginal probability of features in the source and the target domain. Our experiments on several real- world problems show that FLDA performs on par with state- of- the-art domain-adaptation techniques.","tags":["machine-learning","transfer-model"],"title":"Feature-level domain adaptation","type":"publication"},{"authors":null,"categories":null,"content":"NVPHBV is the Dutch Society for Pattern Recognition and Image Processing. During their meetings, researchers from the Netherlands have a chance to present some of their work and catch up on developments in their fields.\nI presented my work on a robust estimator for linear discriminant analysis in domain-adaptive machine learning.\n","date":1464369513,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464369513,"objectID":"ab583c637ab232eef2cef0fe8e38ea0f","permalink":"https://wmkouw.github.io/talk/nvphbv2016/","publishdate":"2016-05-27T19:18:33+02:00","relpermalink":"/talk/nvphbv2016/","section":"talk","summary":"NVPHBV is the Dutch Society for Pattern Recognition and Image Processing. During their meetings, researchers from the Netherlands have a chance to present some of their work and catch up on developments in their fields.\nI presented my work on a robust estimator for linear discriminant analysis in domain-adaptive machine learning.","tags":["machine-learning","domain-adaptation","robust-estimator"],"title":"Target contrastive estimator for robust domain adaptation","type":"talk"},{"authors":null,"categories":null,"content":"\nIn domain adaptation, one is always required to make assumptions on how the two domains relate to each other. However, depending on the domain dissimilarity, these assumptions can be quite strong. Furthermore, they are often hard to support, even with labeled target samples. Unfortunately, when an assumption is invalid, the classifier can adapt itself in ways that are detrimental to performance. Therefore, in practice, it can be hard to predict whether a domain-adaptive classifier will perform well for a given problem setting.\nOur aim was to design a more robust method, one that makes no assumptions on the relationship between the domains, but is still guaranteed to never perform worse than the naive, non-adaptive classifier. We dubbed our resulting parameter estimator the Target Contrastive Pessimistic Risk estimator, after the estimator it was inspired on: Maximum Contrastive Pessimistic Likelihood.\n","date":1462118701,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462118701,"objectID":"f19c417b15d53a56763956cfb21d1914","permalink":"https://wmkouw.github.io/project/tcpr/","publishdate":"2016-05-01T18:05:01+02:00","relpermalink":"/project/tcpr/","section":"project","summary":"Target contrastive robust risk, for safe domain adaptation.","tags":["machine-learning","domain-adaptation","maximum-contrastive-pessimistic-likelihood"],"title":"TCPR","type":"project"},{"authors":null,"categories":null,"content":"ICT.OPEN is the Dutch national conference for computer science in general. Researchers from various universities and companies present their work along different tracks.\nI presented my first project, on feature-level domain adaptation, in the Natural Artificial Intelligence track.\n","date":1458463469,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1458463469,"objectID":"d091db8729fe42c6e9971b53e8032f9d","permalink":"https://wmkouw.github.io/talk/ictopen2016/","publishdate":"2016-03-20T10:44:29+02:00","relpermalink":"/talk/ictopen2016/","section":"talk","summary":"ICT.OPEN is the Dutch national conference for computer science in general. Researchers from various universities and companies present their work along different tracks.\nI presented my first project, on feature-level domain adaptation, in the Natural Artificial Intelligence track.","tags":["machine-learning","domain-adaptation","transfer-model"],"title":"Feature-level domain adaptation","type":"talk"},{"authors":null,"categories":null,"content":"\nSuppose that data is collected according to a set of features, such as blood pressure in clinical research or word frequencies in natural language processing. Now, in the source domain, all these features are measured, while in the target domain, some values are missing or absent (i.e. words are not used in the target context). In this case, one could capture the relationship between the domains using a probabilistic model, an approach we have called feature-level domain adaptation. This \u0026ldquo;transfer model\u0026rdquo; describes the probability of observing a target sample, given that you have observed a particular source sample.\nWe designed a family of classifiers that perform domain adaptation by training with respect to a transfer model. Information dropout, as described in the example above, is one of the simplest and yet most widely applicable class of transfer models. We developed code for dropout transfer and applied it to settings with data missing-not-at-random, inactive regions of image space and word frequency reductions.\n","date":1448985905,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448985905,"objectID":"9a9902be9a039cccd419994d13f7433a","permalink":"https://wmkouw.github.io/project/flda/","publishdate":"2015-12-01T18:05:05+02:00","relpermalink":"/project/flda/","section":"project","summary":"Feature-level domain adaptation using dropout transfer models.","tags":["machine-learning","domain-adaptation","transfer-model","dropout"],"title":"FLDA","type":"project"},{"authors":null,"categories":null,"content":"SNN organized a one day symposium entitled Intelligent Machines, where an overview of recent developments was presented. The meeting aimed to establish a dialogue and to build connections between academic research, industry and public institutions in the Netherlands.\nI presented my preliminary work on incorporating transfer models in domain-adaptive classifiers.\n","date":1426582830,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426582830,"objectID":"14ad9d07fbad26b51cd8ac2c355073ae","permalink":"https://wmkouw.github.io/talk/snn2015/","publishdate":"2015-03-17T11:00:30+02:00","relpermalink":"/talk/snn2015/","section":"talk","summary":"SNN organized a one day symposium entitled Intelligent Machines, where an overview of recent developments was presented. The meeting aimed to establish a dialogue and to build connections between academic research, industry and public institutions in the Netherlands.\nI presented my preliminary work on incorporating transfer models in domain-adaptive classifiers.","tags":["machine-learning","domain-adaptation","transfer-model"],"title":"Feature absence regularization for domain-adaptive learning","type":"talk"}]