[{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1536410735,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536410735,"objectID":"6f99087f7dc10bea02297a292bc00b71","permalink":"https://wmkouw.github.io/publication/tcpr/","publishdate":"2018-09-08T14:45:35+02:00","relpermalink":"/publication/tcpr/","section":"publication","summary":"Domain-adaptive classifiers learn from a source domain and aim to generalize to a target domain. If the classifier's assumptions on the relationship between domains (e.g. covariate shift) are valid, then it will usually outperform a non-adaptive source classifier. Unfortunately, it can perform substantially worse when its assumptions are invalid. Validating these assumptions requires labeled target samples, which are usually not available. We argue that, in order to make domain-adaptive classifiers more practical, it is necessary to focus on robust methods; robust in the sense that the model still achieves a particular level of performance without making strong assumptions on the relationship between domains. With this objective in mind, we formulate a conservative parameter estimator that only deviates from the source classifier when a lower or equal risk is guaranteed for all possible labellings of the given target samples. We derive the corresponding estimator for a discriminant analysis model, and show that its risk is actually strictly smaller than that of the source classifier. Experiments indicate that our classifier outperforms state-of-the-art classifiers for geographically biased samples. ","tags":[],"title":"Target contrastive pessimistic discriminant analysis","type":"publication"},{"authors":["W.M. Kouw","M. Loog","W. Bartels","A.M. Mendrik"],"categories":null,"content":"","date":1536410325,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536410325,"objectID":"515350e69b50ecab6835070f81b623a8","permalink":"https://wmkouw.github.io/publication/mrainet/","publishdate":"2018-09-08T14:38:45+02:00","relpermalink":"/publication/mrainet/","section":"publication","summary":"Voxelwise classification approaches are popular and effective methods for tissue quantification in brain magnetic resonance imaging (MRI) scans. However, generalization of these approaches is hampered by large differences between sets of MRI scans such as differences in field strength, vendor or acquisition protocols. Due to this acquisition related variation, classifiers trained on data from a specific scanner fail or under-perform when applied to data that was acquired differently. In order to address this lack of generalization, we propose a Siamese neural network (MRAI-net) to learn a representation that minimizes the between-scanner variation, while maintaining the contrast between brain tissues necessary for brain tissue quantification. The proposed MRAI-net was evaluated on both simulated and real MRI data. After learning the MR acquisition invariant representation, any supervised classification model that uses feature vectors can be applied. In this paper, we provide a proof of principle, which shows that a linear classifier applied on the MRAI representation is able to outperform supervised convolutional neural network classifiers for tissue classification when little target training data is available. ","tags":[],"title":"MR Acquisition-invariant feature extraction using Siamese neural networks.","type":"publication"},{"authors":null,"categories":null,"content":"Research institutions, hospitals, governments and companies are collecting all kinds of data at an increased rate. Such “big data” offers the possibility to increase our understanding of human behavior, improve clinical diagnosis, and create intelligent consumer products. However, data can also end up in the hands of parties that may take advantage of you. For example, Facebook user data was leaked to political campaigners to influence voters [1]. Central to this debate is the concept of trust: how can you trust that the data that you give out will not end up hurting you?\nThis debate was recently sparked by the adoption of the General Data Protection Regulation (GDPR), which gives individuals in the EU the right to review and destroy personal data collected by third parties. The GDPR greatly complicates the collection and dissemination of research data, particularly in medical sciences. How should researchers analyze data and publish their findings if their subjects decide that their data is to remain private? Can patient data be fully anonymized and yet remain traceable at the same time? And how does data protection and privacy fit within the principle of open science that encourages researchers to make their data Findable, Accessible, Interoperable, and Reusable (FAIR)?\nAn interesting novel technology in the data and trust debate is blockchain. The goal is to create a distributed transaction system without a central authority that profits from this position, for example the exchange of money without the intervention of a bank. Individual users can issue transactions that are subsequently verified by the entire community through a blockchain. If there’s foul play, for instance by someone changing their own balance, the transaction is rejected by the community. Blockchain thus ensures consensus between users that do not trust each other, which makes the technology interesting for a wide range of applications.\nIn this workshop, we will present different views on how big data is currently being used. Furthermore, we will initiate an interactive discussion with the audience during which the participants can answer questions and share opinions using their smartphones.\n[1] https://www.theguardian.com/news/2018/mar/17/cambridge-analytica-facebook-influence-us-election\n","date":1536405486,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536405486,"objectID":"f54be9f9cacfa086dabea59e675e1470","permalink":"https://wmkouw.github.io/talk/nsfworkshop-bigdata/","publishdate":"2018-09-08T13:18:06+02:00","relpermalink":"/talk/nsfworkshop-bigdata/","section":"talk","summary":"Research institutions, hospitals, governments and companies are collecting all kinds of data at an increased rate. Such “big data” offers the possibility to increase our understanding of human behavior, improve clinical diagnosis, and create intelligent consumer products. However, data can also end up in the hands of parties that may take advantage of you. For example, Facebook user data was leaked to political campaigners to influence voters [1]. Central to this debate is the concept of trust: how can you trust that the data that you give out will not end up hurting you?","tags":[],"title":"Big Data \u0026 Trust","type":"talk"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1536404489,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536404489,"objectID":"bfb0bee66da7019e8150e1a9a7bbc281","permalink":"https://wmkouw.github.io/publication/covshift-skew/","publishdate":"2018-09-08T13:01:29+02:00","relpermalink":"/publication/covshift-skew/","section":"publication","summary":"Importance-weighting is a popular and well-researched technique for dealing with sample selection bias and covariate shift. It has desirable characteristics such as unbiasedness, consistency and low computational complexity. However, weighting can have a detrimental effect on an estimator as well. In this work, we empirically show that the sampling distribution of an importance-weighted estimator can be skewed. For sample selection bias settings, and for small sample sizes, the importance-weighted risk estimator produces overestimates for datasets in the body of the sampling distribution, i.e. the majority of cases, and large underestimates for data sets in the tail of the sampling distribution. These over- and underestimates of the risk lead to suboptimal regularization parameters when used for importance-weighted validation. ","tags":[],"title":"Effects of sampling skewness of the importance-weighted risk estimator on model selection","type":"publication"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1480588890,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480588890,"objectID":"fd21f3090c6ff551eade84e32ba85f5f","permalink":"https://wmkouw.github.io/publication/covshift-l2reg/","publishdate":"2016-12-01T12:41:30+02:00","relpermalink":"/publication/covshift-l2reg/","section":"publication","summary":"This paper identifies a problem with the usual procedure for L2-regularization parameter estimation in a domain adaptation setting. In such a setting, there are differences between the distributions generating the training data (source domain) and the test data (target domain). The usual cross-validation procedure requires validation data, which can not be obtained from the unlabeled target data. The problem is that if one decides to use source validation data, the regularization parameter is underestimated. One possible solution is to scale the source validation data through importance weighting, but we show that this correction is not sufficient. We conclude the paper with an empirical analysis of the effect of several importance weight estimators on the estimation of the regularization parameter.","tags":[],"title":"On regularization parameter estimation under covariate shift","type":"publication"},{"authors":["W.M. Kouw","L.J.P. van der Maaten","J.H. Krijthe","M. Loog"],"categories":null,"content":"","date":1477996402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477996402,"objectID":"884d96bfc381538193f86239ebeabb5f","permalink":"https://wmkouw.github.io/publication/flda/","publishdate":"2016-11-01T12:33:22+02:00","relpermalink":"/publication/flda/","section":"publication","summary":"Domain adaptation is the supervised learning setting in which the training and test data are sampled from different distributions: training data is sampled from a source domain, whilst test data is sampled from a target domain. This paper proposes and studies an approach, called feature-level domain adaptation (FLDA), that models the dependence between the two domains by means of a feature-level transfer model that is trained to describe the transfer from source to target domain. Subsequently, we train a domain-adapted classifier by minimizing the expected loss under the resulting transfer model. For linear classifiers and a large family of loss functions and transfer models, this expected loss can be computed or approximated analytically, and minimized efficiently. Our empirical evaluation of FLDA focuses on problems comprising binary and count data in which the transfer can be naturally modeled via a dropout distribution, which allows the classifier to adapt to differences in the marginal probability of features in the source and the target domain. Our experiments on several real- world problems show that FLDA performs on par with state- of- the-art domain-adaptation techniques.","tags":[],"title":"Feature-level domain adaptation","type":"publication"}]