[{"authors":null,"categories":null,"content":"Research institutions, hospitals, governments and companies are collecting all kinds of data at an increased rate. Such “big data” offers the possibility to increase our understanding of human behavior, improve clinical diagnosis, and create intelligent consumer products. However, data can also end up in the hands of parties that may take advantage of you. For example, Facebook user data was leaked to political campaigners to influence voters [1]. Central to this debate is the concept of trust: how can you trust that the data that you give out will not end up hurting you?\nThis debate was recently sparked by the adoption of the General Data Protection Regulation (GDPR), which gives individuals in the EU the right to review and destroy personal data collected by third parties. The GDPR greatly complicates the collection and dissemination of research data, particularly in medical sciences. How should researchers analyze data and publish their findings if their subjects decide that their data is to remain private? Can patient data be fully anonymized and yet remain traceable at the same time? And how does data protection and privacy fit within the principle of open science that encourages researchers to make their data Findable, Accessible, Interoperable, and Reusable (FAIR)?\nAn interesting novel technology in the data and trust debate is blockchain. The goal is to create a distributed transaction system without a central authority that profits from this position, for example the exchange of money without the intervention of a bank. Individual users can issue transactions that are subsequently verified by the entire community through a blockchain. If there’s foul play, for instance by someone changing their own balance, the transaction is rejected by the community. Blockchain thus ensures consensus between users that do not trust each other, which makes the technology interesting for a wide range of applications.\nIn this workshop, we will present different views on how big data is currently being used. Furthermore, we will initiate an interactive discussion with the audience during which the participants can answer questions and share opinions using their smartphones.\n[1] https://www.theguardian.com/news/2018/mar/17/cambridge-analytica-facebook-influence-us-election\n","date":1536405486,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536405486,"objectID":"f54be9f9cacfa086dabea59e675e1470","permalink":"https://wmkouw.github.io/talk/nsfworkshop-bigdata/","publishdate":"2018-09-08T13:18:06+02:00","relpermalink":"/talk/nsfworkshop-bigdata/","section":"talk","summary":"Research institutions, hospitals, governments and companies are collecting all kinds of data at an increased rate. Such “big data” offers the possibility to increase our understanding of human behavior, improve clinical diagnosis, and create intelligent consumer products. However, data can also end up in the hands of parties that may take advantage of you. For example, Facebook user data was leaked to political campaigners to influence voters [1]. Central to this debate is the concept of trust: how can you trust that the data that you give out will not end up hurting you?","tags":["Data","trust","GDPR"],"title":"Big data \u0026 trust","type":"talk"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1536404489,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536404489,"objectID":"bfb0bee66da7019e8150e1a9a7bbc281","permalink":"https://wmkouw.github.io/publication/covshift-skew/","publishdate":"2018-09-08T13:01:29+02:00","relpermalink":"/publication/covshift-skew/","section":"publication","summary":"Importance-weighting is a popular and well-researched technique for dealing with sample selection bias and covariate shift. It has desirable characteristics such as unbiasedness, consistency and low computational complexity. However, weighting can have a detrimental effect on an estimator as well. In this work, we empirically show that the sampling distribution of an importance-weighted estimator can be skewed. For sample selection bias settings, and for small sample sizes, the importance-weighted risk estimator produces overestimates for datasets in the body of the sampling distribution, i.e. the majority of cases, and large underestimates for data sets in the tail of the sampling distribution. These over- and underestimates of the risk lead to suboptimal regularization parameters when used for importance-weighted validation. ","tags":["covariate-shift","cross-validation","sampling-skewness"],"title":"Effects of sampling skewness of the importance-weighted risk estimator on model selection","type":"publication"},{"authors":null,"categories":null,"content":"\nLibTLDA is a library of classifiers designed for domain adaptation and transfer learning, available in Matlab and Python. It started out as the collection of classifiers that I implemented during my PhD, but is now being expanded with more methods and algorithms.\nInstallation has been made easy, and there are demos to help you get started. Coders familiar with sci-kit will find it easy to pick up.\nMore information is available on its documentation page.\n","date":1528564017,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528564017,"objectID":"0ab21211d334a19cc02646aaea399035","permalink":"https://wmkouw.github.io/project/libtlda/","publishdate":"2018-06-09T19:06:57+02:00","relpermalink":"/project/libtlda/","section":"project","summary":"Library of domain-adaptive classifiers and transfer learners.","tags":["machine-learning","domain-adaptation","transfer-learning"],"title":"LibTLDA","type":"project"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1528461935,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528461935,"objectID":"6f99087f7dc10bea02297a292bc00b71","permalink":"https://wmkouw.github.io/publication/tcpr/","publishdate":"2018-06-08T14:45:35+02:00","relpermalink":"/publication/tcpr/","section":"publication","summary":"Domain-adaptive classifiers learn from a source domain and aim to generalize to a target domain. If the classifier's assumptions on the relationship between domains (e.g. covariate shift) are valid, then it will usually outperform a non-adaptive source classifier. Unfortunately, it can perform substantially worse when its assumptions are invalid. Validating these assumptions requires labeled target samples, which are usually not available. We argue that, in order to make domain-adaptive classifiers more practical, it is necessary to focus on robust methods; robust in the sense that the model still achieves a particular level of performance without making strong assumptions on the relationship between domains. With this objective in mind, we formulate a conservative parameter estimator that only deviates from the source classifier when a lower or equal risk is guaranteed for all possible labellings of the given target samples. We derive the corresponding estimator for a discriminant analysis model, and show that its risk is actually strictly smaller than that of the source classifier. Experiments indicate that our classifier outperforms state-of-the-art classifiers for geographically biased samples. ","tags":["domain-adaptation","estimator","robustness"],"title":"Target contrastive pessimistic discriminant analysis","type":"publication"},{"authors":null,"categories":null,"content":"NVPHBV is the Dutch Society for Pattern Recognition and Image Processing. During their meetings, researchers from the Netherlands have a chance to present some of their work and catch up on developments in their fields.\nI presented my work on removing MRI-scanner based varation from images.\n","date":1525887531,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525887531,"objectID":"9ea9a9a81abfe8ad41df86e23ae80ce8","permalink":"https://wmkouw.github.io/talk/nvphbv2018/","publishdate":"2018-05-09T19:38:51+02:00","relpermalink":"/talk/nvphbv2018/","section":"talk","summary":"NVPHBV is the Dutch Society for Pattern Recognition and Image Processing. During their meetings, researchers from the Netherlands have a chance to present some of their work and catch up on developments in their fields.\nI presented my work on removing MRI-scanner based varation from images.","tags":["representation-learning","feature-extraction","MRI"],"title":"MR Acquisition-invariant representation learning","type":"talk"},{"authors":null,"categories":null,"content":"\nWhile I was at the Netherlands eScience Center, I worked on the Young eScientist Award 2016 project, together with its winner Maureen van Eijnatten and PhD student Jordi Minnema. They\u0026rsquo;re part of the 3D Innovation Lab under the department of Maxillo-Facial Surgery at the Vrije Universiteit medical center.\nDuring the project, we developed a method for automatically detecting bone tissue in medical CT-scanner images. This method consisted of an artificial neural network, called a convolutional neural network (CNN), which looks at individual voxels in the image plus their surroundings. If a voxel shows an intensity value suitable for bone tissue and is surrounded voxels with similar intensity values (noisy signal voxels are often isolated), then the network classifies that voxel as bone. It performs its function for a whole CT-scan, which allows us to automatically reconstruct a 3D model of the skull. This 3D model is later used to design and 3D-print saw templates and other constructs to aid surgeons.\n","date":1504977174,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504977174,"objectID":"c43a9b8e6c0d6ff24287516d1a140c59","permalink":"https://wmkouw.github.io/project/mfs-bonecnn/","publishdate":"2017-09-09T19:12:54+02:00","relpermalink":"/project/mfs-bonecnn/","section":"project","summary":"Bone tissue recognition from CT-scans of maxillo-facial surgery patients.","tags":["deep-learning","medical-imaging","maxillo-facial-surgery"],"title":"   bone-CTCNN","type":"project"},{"authors":["W.M. Kouw","M. Loog","W. Bartels","A.M. Mendrik"],"categories":null,"content":"","date":1504874325,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504874325,"objectID":"515350e69b50ecab6835070f81b623a8","permalink":"https://wmkouw.github.io/publication/mrainet/","publishdate":"2017-09-08T14:38:45+02:00","relpermalink":"/publication/mrainet/","section":"publication","summary":"Voxelwise classification approaches are popular and effective methods for tissue quantification in brain magnetic resonance imaging (MRI) scans. However, generalization of these approaches is hampered by large differences between sets of MRI scans such as differences in field strength, vendor or acquisition protocols. Due to this acquisition related variation, classifiers trained on data from a specific scanner fail or under-perform when applied to data that was acquired differently. In order to address this lack of generalization, we propose a Siamese neural network (MRAI-net) to learn a representation that minimizes the between-scanner variation, while maintaining the contrast between brain tissues necessary for brain tissue quantification. The proposed MRAI-net was evaluated on both simulated and real MRI data. After learning the MR acquisition invariant representation, any supervised classification model that uses feature vectors can be applied. In this paper, we provide a proof of principle, which shows that a linear classifier applied on the MRAI representation is able to outperform supervised convolutional neural network classifiers for tissue classification when little target training data is available. ","tags":["representation-learning","feature-extraction","MRI"],"title":"MR Acquisition-invariant feature extraction using Siamese neural networks.","type":"publication"},{"authors":null,"categories":null,"content":"\nSuppose you have two sets of MR images, that were acquired using different scanners and/or different scanning protocols. One set of images is brighter and shows more tissue contrast than the other. One of them has annotations and the other does not. Tissue classifiers \u0026ndash; important to computer-aided diagnosis systems \u0026ndash; can be trained on the annotated data from one scanner, but when applied to data from the other scanner they will drastically under-perform. It is exactly what makes computer systems so powerful, i.e. looking at minute variations in pixel intensities, that also makes them vulnerable to data set shifts.\nWe developed a method called MR Acquisition-Invariant Neural Network that aims to learn a representation of patches such that these show minimal variation with respect to the MRI scanner without losing clinically-relevant tissue variation. Once trained, the network extracts acquisition-invariant feature vectors, which can be used for a variety of tasks in medical images later on.\nMore information is available on its documentation page.\n","date":1496333089,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496333089,"objectID":"60b752f8e01c22e789347b81fe65961e","permalink":"https://wmkouw.github.io/project/mrainet/","publishdate":"2017-06-01T18:04:49+02:00","relpermalink":"/project/mrainet/","section":"project","summary":"MR acquisition-invarant representation learning using Siamese neural networks.","tags":["MRI","acquisition-variation","representation-learning"],"title":"MRAI-net","type":"project"},{"authors":null,"categories":null,"content":"ICT.OPEN is the Dutch national conference for computer science in general. Researchers from various universities and companies present their work along different tracks.\nI presented my work in the Machine Learning track. My topic was cross-validation under covariate shift and how to reduce variance in the importance-weighted risk estimator.\n","date":1489079804,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489079804,"objectID":"3b58fc47ee57e9092bf91cdd84b9eb61","permalink":"https://wmkouw.github.io/talk/ictopen2017/","publishdate":"2017-03-09T19:16:44+02:00","relpermalink":"/talk/ictopen2017/","section":"talk","summary":"ICT.OPEN is the Dutch national conference for computer science in general. Researchers from various universities and companies present their work along different tracks.\nI presented my work in the Machine Learning track. My topic was cross-validation under covariate shift and how to reduce variance in the importance-weighted risk estimator.","tags":["machine-learning","covariate-shift","importance-weighting","variance-reduction"],"title":"Variance reduction techniques for importance-weighted cross-validation","type":"talk"},{"authors":["W.M. Kouw","M. Loog"],"categories":null,"content":"","date":1480588890,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480588890,"objectID":"fd21f3090c6ff551eade84e32ba85f5f","permalink":"https://wmkouw.github.io/publication/covshift-l2reg/","publishdate":"2016-12-01T12:41:30+02:00","relpermalink":"/publication/covshift-l2reg/","section":"publication","summary":"This paper identifies a problem with the usual procedure for L2-regularization parameter estimation in a domain adaptation setting. In such a setting, there are differences between the distributions generating the training data (source domain) and the test data (target domain). The usual cross-validation procedure requires validation data, which can not be obtained from the unlabeled target data. The problem is that if one decides to use source validation data, the regularization parameter is underestimated. One possible solution is to scale the source validation data through importance weighting, but we show that this correction is not sufficient. We conclude the paper with an empirical analysis of the effect of several importance weight estimators on the estimation of the regularization parameter.","tags":[],"title":"On regularization parameter estimation under covariate shift","type":"publication"},{"authors":["W.M. Kouw","L.J.P. van der Maaten","J.H. Krijthe","M. Loog"],"categories":null,"content":"","date":1477996402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477996402,"objectID":"884d96bfc381538193f86239ebeabb5f","permalink":"https://wmkouw.github.io/publication/flda/","publishdate":"2016-11-01T12:33:22+02:00","relpermalink":"/publication/flda/","section":"publication","summary":"Domain adaptation is the supervised learning setting in which the training and test data are sampled from different distributions: training data is sampled from a source domain, whilst test data is sampled from a target domain. This paper proposes and studies an approach, called feature-level domain adaptation (FLDA), that models the dependence between the two domains by means of a feature-level transfer model that is trained to describe the transfer from source to target domain. Subsequently, we train a domain-adapted classifier by minimizing the expected loss under the resulting transfer model. For linear classifiers and a large family of loss functions and transfer models, this expected loss can be computed or approximated analytically, and minimized efficiently. Our empirical evaluation of FLDA focuses on problems comprising binary and count data in which the transfer can be naturally modeled via a dropout distribution, which allows the classifier to adapt to differences in the marginal probability of features in the source and the target domain. Our experiments on several real- world problems show that FLDA performs on par with state- of- the-art domain-adaptation techniques.","tags":["machine-learning","transfer-models"],"title":"Feature-level domain adaptation","type":"publication"},{"authors":null,"categories":null,"content":"NVPHBV is the Dutch Society for Pattern Recognition and Image Processing. During their meetings, researchers from the Netherlands have a chance to present some of their work and catch up on developments in their fields.\nI presented my work on a robust estimator for linear discriminant analysis in domain-adaptive machine learning.\n","date":1464369513,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464369513,"objectID":"ab583c637ab232eef2cef0fe8e38ea0f","permalink":"https://wmkouw.github.io/talk/nvphbv2016/","publishdate":"2016-05-27T19:18:33+02:00","relpermalink":"/talk/nvphbv2016/","section":"talk","summary":"NVPHBV is the Dutch Society for Pattern Recognition and Image Processing. During their meetings, researchers from the Netherlands have a chance to present some of their work and catch up on developments in their fields.\nI presented my work on a robust estimator for linear discriminant analysis in domain-adaptive machine learning.","tags":["machine-learning","domain-adaptation","robust-estimator"],"title":"Target contrastive estimator for robust domain adaptation","type":"talk"},{"authors":null,"categories":null,"content":"\nIn domain adaptation, one is always required to make assumptions on how the two domains relate to each other. However, depending on the domain dissimilarity, these assumptions can be quite strong. Furthermore, they are often hard to support, even with labeled target samples. Unfortunately, when an assumption is invalid, the classifier can adapt itself in ways that are detrimental to performance. Therefore, in practice, it can be hard to predict whether a domain-adaptive classifier will perform well for a given problem setting.\nOur aim was to design a more robust method, one that makes no assumptions on the relationship between the domains, but is still guaranteed to never perform worse than the naive, non-adaptive classifier. We dubbed our resulting parameter estimator the Target Contrastive Pessimistic Risk estimator, after the estimator it was inspired on: Maximum Contrastive Pessimistic Likelihood.\n","date":1462118701,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462118701,"objectID":"f19c417b15d53a56763956cfb21d1914","permalink":"https://wmkouw.github.io/project/tcpr/","publishdate":"2016-05-01T18:05:01+02:00","relpermalink":"/project/tcpr/","section":"project","summary":"Target contrastive pessimistic risk, for safe domain adaptation.","tags":["machine-learning","domain-adaptation","maximum-contrastive-pessimistic-likelihood"],"title":"TCPR","type":"project"},{"authors":null,"categories":null,"content":"\nSuppose that data is collected according to a set of features, such as blood pressure in clinical research or word frequencies in natural language processing. Now, in the source domain, all these features are measured, while in the target domain, some values are missing or absent (i.e. words are not used in the target context). In this case, one could capture the relationship between the domains using a probabilistic model, an approach we have called feature-level domain adaptation. This \u0026ldquo;transfer model\u0026rdquo; describes the probability of observing a target sample, given that you have observed a particular source sample.\nWe designed a family of classifiers that perform domain adaptation by training with respect to a transfer model. Information dropout, as described in the example above, is one of the simplest and yet most widely applicable class of transfer models. We developed code for dropout transfer and applied it to settings with data missing-not-at-random, inactive regions of image space and word frequency reductions.\n","date":1448985905,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448985905,"objectID":"9a9902be9a039cccd419994d13f7433a","permalink":"https://wmkouw.github.io/project/flda/","publishdate":"2015-12-01T18:05:05+02:00","relpermalink":"/project/flda/","section":"project","summary":"Feature-level domain adaptation, using dropout transfer models.","tags":["machine-learning","domain-adaptation","transfer-model","dropout"],"title":"FLDA","type":"project"}]